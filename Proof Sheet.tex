\documentclass[hidelinks,11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amsfonts}
\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage[mathscr]{eucal}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage[OT1]{fontenc}
\usepackage{physics}
\usepackage{tikz-cd}
\usepackage{xpatch}
\usepackage{nicefrac}
\usepackage{mathtools}
\usepackage{environ}

\setlength\parindent{0pt}

\newtheoremstyle{dotless}{10pt}{10pt}{\upshape}{}{\bfseries}{}{ }{}

\theoremstyle{definition}
\newtheorem*{defin}{DEF}

\theoremstyle{dotless}
\newtheorem{prop}{Proof}[section]
\newtheorem*{corollary}{Corollary}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\usepackage{hyperref}
\hypersetup{colorlinks=false}

\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\R}{\mathbb{R}}
\DeclareMathOperator{\1}{\mathbf{1}}



\begin{document}
\begin{center}
{\Large\textbf STAT 381-383-385 (2021) \hspace{0.1cm} Proof Page}\medbreak
\large{Alex Sheng}
\end{center}

\section{Measure Theory}

\subsection{Measures}

\begin{prop}
Suppose $\mu$ is downward continuous. Let $\{E_i\}_{i\geq1}$ be a collection of disjoint sets and $E=\amalg_{i\geq1}E_i$. Construct a decreasing sequence of sets $\{F_i\}_{i\geq1}$ by letting
$F_n=E\setminus\amalg_{i=1}^nE_i$ for all $n\geq1$, then by downward continuity
\[\mu(E)-\lim_{n\to\infty}\mu(\coprod_{i=1}^nE_i)=\lim_{n\to\infty}\mu(F_n)=\mu(\bigcap_{i\geq1}F_i)=0.\]
Then by finite additivity, $\mu(E)=\lim_{n\to\infty}\mu(\amalg_{i=1}^nE_i)=\lim_{n\to\infty}\sum_{i=1}^n\mu(E_i)=\sum_{i\geq1}\mu(E_i)$.\medbreak
On the other hand suppose $\mu$ is $\sigma$-additive.
Let $\{A_i\}_{i\geq1}$ be an increasing sequence of sets and $A=\cup_{i\geq1}A_i$. Construct a collection $\{B_i\}_{i\geq1}$ of disjoint sets by letting $B_1=A_1$ and $B_n=A_n\setminus A_{n-1}$ for all $n\geq2$, then by $\sigma$-additivity
\begin{equation}\label{(1)}
    \lim_{n\to\infty}\mu(A_n)=\lim_{n\to\infty}\mu(\coprod_{i=1}^nB_i)=\mu(\coprod_{i\geq1}B_i)=\mu(A).
\end{equation}
Now let $\{C_i\}_{i\geq1}$ be a decreasing sequence of sets and $C=\cap_{i\geq1}C_i$. Construct an increasing sequence of sets $\{D_i\}_{i\geq1}$ by letting $D_n=C_1\setminus C_{n+1}$ for all $n\geq1$, then by \hyperref[(1)]{(1)}
\[\mu(C_1)-\lim_{n\to\infty}\mu(C_n)=\lim_{n\to\infty}(D_n)=\mu(C_1\setminus C)=\mu(C_1)-\mu(C).\]
Hence $\mu(C)=\lim_{n\to\infty}\mu(C_n)$.
\end{prop}

\begin{prop}
By definition $\varnothing\in\mathcal{F}$ and $A\in\mathcal{F}\Rightarrow A^c\in\mathcal{F}$. Let $B,C\in\mathcal{F}$ and $D=A\cup B$. Then by countably subadditivity, for any $E\subset\Omega$
\begin{align*}
\mu(E\cap D)+\mu(E\cap D^c)&=\mu((E\cap B)\cup(E\cap C\cap B^c))+\mu(E\cap B^c\cap C^c)\\
&\leq\mu(E\cap B)+\mu(E\cap C\cap B^c)+\mu(E\cap B^c\cap C^c)
\end{align*}
Since $C\in\mathcal{F}$, $\mu(E\cap C\cap B^c)+\mu(E\cap B^c\cap C^c)=\mu(E\cap B^c)$. And since $B\in\mathcal{F}$, $\mu(E\cap B)+\mu(E\cap B^c)=\mu(E)$. Hence $\mathcal{F}$ is stable under finite union.\medbreak
Now let $\{A_i\in\mathcal{F}\}_{i\geq1}$, $B_n=\cup_{i=1}^nA_i$, and $B=\cup_{i\geq1}A_i$. Since $B_n\in\mathcal{F}$, take any $E\subset\Omega$ and by monotonicity of $\mu$,
\[\mu(E)=\mu(E\cap B_n)+\mu(E\cap B_n^c)\geq\mu(E\cap B_n)+\mu(E\cap B^c).\]
Construct a collection of disjoint sets $\{C_i\in\mathcal{F}\}_{i\geq1}$ by letting $C_1=A_1$ and $C_n=A_n\setminus\cup_{i=1}^{n-1}A_i$ for all $n\geq2$. Then $B_n=\amalg_{i=1}^nC_i$, and
\begin{align}\label{(2)}
\mu(E\cap B_n)=\mu(E\cap B_n\cap C_n)+\mu(E\cap B_n\cap C_n^c)=\mu(E\cap C_n)+\mu(E\cap B_{n-1}).
\end{align}
By induction $\mu(E\cap B_n)=\sum_{i=1}^n\mu(E\cap C_i)$. Then as $n\to\infty$, $\mu(E\cap B_n)\geq\mu(E\cap B)$ follows from the countable subadditivity of $\mu$. Hence from \hyperref[(2)]{(2)}, $\mu(E)\geq\mu(E\cap B)+\mu(E\cap B^c)$, and finally from countable subadditivity of $\mu$, $B\in\mathcal{F}$.
\medbreak
To check that $\mu$ is a measure on $\mathcal{F}$, note that $\mu(B)\geq\mu(B_n)=\sum_{i=1}^n\mu(C_i)$. As $n\to\infty$ this gives one inequality, and the countable subadditivity of $\mu$ gives the other.
\end{prop}

\begin{prop}
Let $\lambda(\Pi)$ be the smallest $\lambda$ system containing $\Pi$. Define $G_A=\{B:A\cap B\in\lambda(\Pi)\}$ for $A\in\lambda(\Pi)$, then $G_A$ is a $\lambda$-system for any $A\in\lambda(\Pi)$:\begin{itemize}
    \item $X\cap A=A\in\lambda(\Pi)$,
    \item $B\cap A\in\lambda(\Pi)\Rightarrow B^c\cap A=((B\cap A)\cup A^c)^c\in\lambda(\Pi)$, and
    \item $A_i\cap A\in\lambda(\Pi)\Rightarrow(\amalg_{i\geq1}A_i)\cap A=\amalg_{i\geq1}(A_i\cap A)\in\lambda(\Pi)$.
\end{itemize}
If $A,B\in\Pi$, then $A\cap B\in\lambda(\Pi)$ since $\Pi$ is stable under intersection. Hence $\Pi\subset G_A$. Since $\lambda(\Pi)$ is the smallest $\lambda$-system containing $\Pi$, we have $\lambda(\Pi)\subset G_A$. Hence $A\cap C\in\lambda(\Pi)$ for $C\in\lambda(\Pi)$. By the same reasoning we conclude that $\lambda(\Pi)\subset G_C$. Therefore, for any $D\in\lambda(\Pi)$ we have $C\cap D\in\lambda(\Pi)$. In other words, $\lambda(\Pi)$ is stable under intersection.\medbreak
Since $A\setminus B=A\cap B^c$, $\lambda(\Pi)$ is also stable under set-difference, with which we can extend the countable $\amalg$-stability to countable $\cup$-stability. Hence $\lambda(\Pi)$ is a $\sigma$-algebra.
\end{prop}

\begin{prop}
r
\end{prop}

\subsection{Integration}

\subsection{Measure and integration on product spaces}

\section{Basic Probability}

\subsection{Basic notions}

\begin{prop}
See Rudin, it's crystal clear
\end{prop}

\subsection{Independence and tail events}

\begin{prop}
Let $A_1\in\Pi_1$ and define measures $\mu_1(A)=\mathbb{P}(A\cap A_1)$ and $\mu_2(A)=\mathbb{P}(A)\mathbb{P}(A_1)$. Since $\Pi_1$ and $\Pi_2$ are independent, $\mu_1=\mu_2$ holds on $\Pi_2$. By Dynkin's $\pi$-$\lambda$ Theorem $\mu_1=\mu_2$ holds on $\mathcal{A}_2$. Now for $A_2\in\mathcal{A}_2$ measures $\mu_1'(A)=\mathbb{P}(A\cap A_2)=\mathbb{P}(A)\mathbb{P}(A_2)=\mu_2'(A)$ on $\Pi_1$, hence again by Dynkin's $\pi$-$\lambda$ Theorem $\mu_1'=\mu_2'$ on $\mathcal{A}_1$, and the result follows.\medbreak
$\Pi_i=\{\{X_i\leq t\}:t\in\overline{\mathbb{R}}\}$ is a $\pi$-system since $\{X_i\leq s\}\cap\{X_i\leq t\}=\{X_i\leq\min{s,t}\}\in\Pi_i$. The result follows from the previous claim.\medbreak
The claim follows from the fact that $\sigma(f(X))\subset\sigma(X)$ for all random variables $X$ and measurable $f:\mathbb{R}\to\mathbb{R}$.
\end{prop}

\begin{prop}
For rectangle $A_1\times A_2$, by Independence assumption,
\[\mathbb{P}((X_1,X_2)\in A_1\times A_2)=\mathbb{P}(X_1\in A_1\textrm{ and }X_2\in A_2)=\mu_1(A_1)\mu_2(A_2)=\mu_1\times\mu_2(A_2\times A_2).\]
The result follows from Dynkin's $\pi$-$\lambda$ Theorem since rectangles form generating $\pi$-systems.\medbreak
Apply Fubini-Tonelli Theorem on $f(x,y)=xy$:
\[\E(XY)=\int f(x,y)\,d(\mu_X(x)\mu_Y(y))=\int X\,d\mu_X\int Y\,d\mu_Y=\E(X)\E(Y).\]
\end{prop}

\begin{prop}
The proof depends on so-called Hoeffding lemma: If $X\in[a,b]$ almost surely, then for any $\lambda\in\mathbb{R}$,
\[\E(e^{\lambda X})\leq\exp{\lambda\E(X)+\frac{\lambda^2(b-a)^2}{8}}.\]
Indeed,\medbreak
With this, we proceed by using Markov's inequality and the independence assumption:
\begin{align*}
    \mathbb{P}\left(\sum_{i=1}^n(X_i-\E(X_i))\geq t\right)&=\mathbb{P}\left(\exp{\lambda\sum_{i=1}^n(X_i-\E(X_i))}\geq e^{\lambda t}\right)\\
    &\leq e^{-\lambda t}\E\left[\exp{\lambda\sum_{i=1}^n(X_i-\E(X_i))}\right]\leq e^{\lambda t}\exp{\frac{-8}{\lambda^2\sum_{i=1}^n(b_i-a_i)^2}}.
\end{align*}
The result follows by minimizing $\lambda>0$
\end{prop}

\begin{prop}
Count $N(\omega)=\sum_{n=1}^\infty\1_{\{\omega\in A_n\}}$, then by Fubini-Tonelli Theorem,
\[\E[N(\omega)]=\sum_{n=1}^\infty\mathbb{P}(\omega\in A_n)<\infty.\] Hence by properties of integration $N(\omega)<\infty$ almost surely, and the result follows.\medbreak
Conversely assume
\end{prop}

\begin{prop}
Let $\mathcal{F}_n=\sigma(X_1,X_2,\cdots,C_n)$ and $\mathcal{F}_\infty=\sigma(X_1,X_2,\cdots)$, then $A\in\mathcal{T}\subset\mathcal{F}_\infty$. For $B\in\mathcal{F}_n$ consider measures $\mu_1(B)=\mathbb{P}(A)\mathbb{P}(B)$ and $\mu_2(B)=\mathbb{P}(A\cap B)$, then by definition $\mu_1=\mu_2$ on $\cup_{n\geq1}\mathcal{F}_n$. Being a $\pi$-system, $\cup_{n\geq1}\mathcal{F}_n$ generates $\mathcal{F}_\infty$, hence by Dynkin's $\pi$-$\lambda$ Theorem $\mu_1=\mu_2$ on $\mathcal{F}_\infty$. In particular, $\mathbb{P}(A\cap A)=\mathcal{P}^2(A)$ implies that $\mathbb{P}(A)=0$ or $1$.
\end{prop}

\subsection{Convergence of random variables}

\begin{prop}
Let $X_n\xrightarrow{\mathbb{P}}X$. Take any bounded and uniformly continuous $f:\mathcal{S}\to\mathbb{R}$ and $\epsilon>0$, then there exists $\delta>0$ such that $\rho(x,y)<\delta$ implies $\abs{f(x)-f(y)}<\epsilon$. Since $X_n\xrightarrow{\mathbb{P}}X$, we conclude that $f(X_n)\xrightarrow{\mathbb{P}}f(X)$. Since $f$ is bounded, $f(X_n)\xrightarrow{L^1}f(x)$. By Portemanteau lemmas $X_n\xrightarrow{d}X$.\medbreak
For some constant $c$ let $X_n\xrightarrow{d}c$. Take any $\epsilon>0$ and closed set $F=\{x\in\mathcal{S}:\rho(x,c)\geq\epsilon\}$. By Portemanteau lemma
\[\limsup_{n\to\infty}\mu_n(F)=\limsup_{n\to\infty}\mathbb{P}(X_n\in F)\leq\mathbb{P}(X\in F)=\mu(F)=0.\]
Hence $X_n\xrightarrow{\mathbb{P}}c$.\medbreak
r
\end{prop}

\begin{prop}
Take any bounded and uniformly continuous function $f:\mathcal{S}\times\mathcal{S}\to\mathbb{R}$ and $\epsilon>0$, then there exists $\delta>0$ such that $\rho(x,y)<\delta$ implies $\abs{f(x)-f(y)}<\epsilon$. Since $(X_n,Y_n)\xrightarrow{\mathbb{P}}(X_n,c)$, we conclude that $f(X_n,Y_n)\xrightarrow{\mathbb{P}}f(X_n,c)$. Since $f$ is bounded, $f(X_n,Y_n)\xrightarrow{L^1}f(X_n,c)$. Also since $X_n\xrightarrow{d}X$, $f(X_n,c)\xrightarrow{L^1}f(X,c)$. Hence $f(X_n,Y_n)\xrightarrow{L^1}f(X,c)$, and by Portemanteau lemmas $(X_n,Y_n)\xrightarrow{d}(X,c)$. The result then follows.
\end{prop}

\begin{prop}
If $\mu_n\xrightarrow{weak}\mu$ then by definition
\[\int_\mathcal{S}f\,d\mu_n\to\int_\mathcal{S}f\,d\mu\]
for all bounded Lipschitz continuous function $f:\mathcal{S}\to\mathbb{R}$.\medbreak
Suppose these holds, then take any closed set $F\subset\mathcal{S}$ and define $f(x)=\rho(x,F)=\inf_{y\in F}\rho(x,y)$. Then $f$ is indeed Lipschitz: take any $x,x'\in\mathcal{S}$ and $y\in F$ such that $\rho(y,x')\leq\rho(y',x')$ for all $y'\in F$, then $f(x)\leq\rho(x,y)\leq\rho(x,x')+f(x')$ and similarly $f(x')\leq\rho(x,y)\leq\rho(x,x')+f(x)$. Define $g_k(x)=\max(0,1-kf(x))$, which is Lipschitz continuous and bounded by $[0,1]$. Observe that since $F$ is closed, $F(x)=0$ if and only if $x\in F$. Therefore $\1_F\leq g_k$ and $g_k\to\1_F$. Then
\[\limsup_{n\to\infty}\mu_n(F)\leq\limsup_{n\to\infty}\int_\mathcal{S}g_k\,d\mu_n=\int_\mathcal{S}g_k\,d\mu\]
for all $k$, by Dominated Convergence Theorem and the fact that $\mu_n\xrightarrow{weak}\mu$. Take $k\to\infty$ and by Dominated Convergence Theorem again, $\limsup_{n\to\infty}\mu_n(F)\leq\mu(F)$. Take $F^c$ and consider $g'_k=(kf(x)-1)/kf(x)\leq\1_{F^c}$, and the property for open sets hold likewise.\medbreak
Combine these two properties and take measurable $A\subset\mathcal{S}$ with $\mu(\partial A)=0$, then $\mu(A)=\mu(\overline{A})=\mu(A^\circ)$. Hence
\[\mu(A)=\mu(A^\circ)\leq\liminf_{n\to\infty}\mu_n(A^\circ)\leq\limsup_{n\to\infty}\mu_n(\overline{A})\leq\mu(\overline{A})\leq\mu(A)\]
which proves the statement.\medbreak
Lastly, assume the above property holds. We are content to prove the property for $f\in[0,1]$ due to linearity. Denote the set of discontinuity points of $f$ as $D_f$ and $A_t=\{x:f(x)\geq t\}$ for any $t\in[0,1]$. Thus $\partial A_t\subset D_f\cup\{x:f(x)=t\}$. By continuity of measure we conclude that $\mu(\{x:f(x)=t\})=0$, and $\mu(D_f)=0$ by the fact that $f$ is continuous almost everywhere. Hence $\mu(\partial A)=0$. Then by Dominated Convergence Theorem and what we just proved:
\[\int_\mathcal{S}f\,d\mu_n=\int_0^1\mu_n(A_t)\,dt\to\int_0^1\mu(A_t)\,dt=\int_\mathcal{S}f\,d\mu.\]
From which obviously $\mu_n\xrightarrow{weak}\mu$, which completes the cycle of proof.
\end{prop}

\subsection{Laws of large numbers}

\begin{prop}
Since $X_n$ has finite variance for all $n$, $\Var(\sum_{i=1}^nX_n)\leq nc$ for some constant c. Then by uncorrelated assumption
\[\E\left[\left(\frac{1}{n}\sum_{i=1}^nX_i-\E(X_1)\right)\right]=\Var\left(\frac{1}{n}\sum_{i=1}^nX_i\right)=\frac{1}{n^2}\Var\left(\sum_{i=1}^nX_i\right)\leq\frac{c}{n}\to0\]
as $n\to\infty$, and we are done.
\end{prop}

\begin{prop}
Denote $S_n=\sum_{k=1}^nX_{n,k}$, $\overline{S}_n=\sum_{k=1}^n\overline{X}_{n,k}$, and $\mu_n=\sum_{k=1}^n\E(\overline{X}_{n,k})$. For any $\epsilon>0$,
\[\mathbb{P}\left({\frac{S_n-\mu_n}{a_n}}>\epsilon\right)\leq\mathbb{P}(S_n\neq\overline{S}_n)+\mathbb{P}\left({\frac{\overline{S}_n-\mu_n}{a_n}}>\epsilon\right).\]
The first term is bounded by the first assumption:
\[\mathbb{P}\left(S_n\neq\overline{S}_n\right)=\mathbb{P}\left(\bigcup_{k=1}^n\{X_{n,k}\neq\overline{X}_{n,k}\}\right)\leq\sum_{k=1}^n\mathbb{P}\left(\abs{X_{n,k}}>a_n\right)\to0.\]
The second term is bounded by Chebyshev's inequality, independence, and the second assumption:
\begin{align*}
\mathbb{P}\left({\frac{\overline{S}_n-\mu_n}{a_n}}>\epsilon\right)&\leq\frac{1}{\epsilon^2}\E\left(\abs{\frac{\overline{S}_n-\mu_n}{a_n}}^2\right)\\
&=\frac{1}{\epsilon^2a_n^2}\Var(\overline{S}_n)=\frac{1}{\epsilon^2a_n^2}\sum_{k=1}^n\Var(\overline{X}_{n,k})\leq\frac{1}{\epsilon^2a_n^2}\sum_{k=1}^n\mu_n\to0.
\end{align*}
and the result follows.
\end{prop}

\begin{prop}
weak law
\end{prop}

\begin{prop}
The proof is due to Etemadi (1981). As a lemma, $y\sum_{k>y}k^{-2}\leq2$ for $y\geq0$.\medbreak
Assume for simplicity that $X_i\geq0$. Let $k_n=\left\lfloor{\alpha^n}\right\rfloor$ for some arbitrary $\alpha>1$, $Y_i=X_i\textbf{1}_{\{X_i<i\}}$, and $Z_n=\frac{1}{n}\sum_{i=1}^n(Y_i-\E(Y_i))$.\medbreak
By Chebyshev's inequality,  Fubini-Tonelli Theorem, and the fact that $\alpha^n\leq2\left\lfloor{\alpha^n}\right\rfloor$,
\begin{align*}
\sum_{n=1}^\infty\mathbb{P}(\abs{Z_{k_n}}>\epsilon)&\leq\epsilon^{-2}\sum_{n=1}^\infty k_n^{-2}\sum_{i=k_1}^{k_n}\Var(Y_i)\\
&=\epsilon^{-2}\sum_{i=1}^\infty\Var(Y_i)\sum_{n:k_n\geq i}k_n^{-2}\leq4(1-\alpha^{-2})^{-1}\epsilon^{-2}\sum_{i=1}^\infty i^{-2}\Var(Y_i).
\end{align*}
Since by Fubini-Tonelli Theorem, the lemma, and the assumption $\E(\abs{X_i})<\infty$
\[\sum_{i=1}^\infty i^{-2}\Var(Y_i)\leq\sum_{i=1}^\infty i^{-2}\E(Y_i^2)\leq2\int_0^\infty\,dy\left\{\sum_{i=1}^\infty i^{-2}\textbf{1}_{\{y<k\}}\right\}y\mathbb{P}(\abs{X_1}>y)\leq4\E(\abs{X_i})<\infty,\]
we conclude that $\mathbb{P}(\abs{Z_{n_k}}>\epsilon)\to0$, or $k_n^{-1}\sum_{i=k_1}^{k_n}Y_i\to k_n^{-1}\sum_{i=k_1}^{k_n}\E(Y_i)$. To deal with middle values note that for $k_n<m<k_{n+1}$
\[k_{n+1}^{-1}\sum_{i=1}^{k_n}Y_i\leq m^{-1}\sum_{i=1}^mY_i\leq k_n^{-1}\sum_{i=k_1}^{k_{n+1}}Y_i.\]
Hence $n^{-1}\sum_{i=1}^nY_i\to n^{-1}\sum_{i=1}^n\E(Y_i)$.\medbreak
Since $\sum_{i=1}^\infty\mathbb{P}(X_i\neq Y_i)=\sum_{i=1}^\infty\mathbb{P}(X_1\geq i)\leq\E(X_1)<\infty$, by Borel Cantelli Lemma $\mathbb{P}(X_i\neq Y_i,\textrm{ infinitely often})=0$, which implies that $n^{-1}\sum_{i=1}^nX_i\xrightarrow{a.s.}n^{-1}\sum_{i=1}^nY_i$.\medbreak
Lastly by Dominated Convergence Theorem $\E(Y_i)-\mu=\E(X_1\textbf{1}_{\{X_i\geq i\}})\to0$, which implies that $n^{-1}\sum_{i=1}^n\E(Y_i)\to\mu$. Combining these result and the statement follows:
\[n^{-1}\sum_{i=1}^nX_i\xrightarrow{a.s.}n^{-1}\sum_{i=1}^nY_i\to n^{-1}\sum_{i=1}^n\E(Y_i)\to\mu\]
\end{prop}

\subsection{Tightness and characteristic functions}

\begin{prop}
Suppose first that $\{X_n\}$ is a tight family. Let $F_n$ be the distribution function of $X_n$. Since $F_n$ is bounded, there is a subsequence $F_{n_k}$ that converges pointwise $F_\ast(q)=\lim_{k\to\infty}F_{n_k}(q)$. Define $F(x)=\inf\{F_\ast(q):q>x\}$. We will show $F(x)=\lim_{k\to\infty}F_{n_k}(x)$ is a distribution function.\medbreak
In order for $F(x)$ to be a distribution function, we only need to check the criterion in prop xxx. The monotonicity is inherited. $F(x)\to0$ as $x\to-\infty$ and $F(x)\to1$ as $x\to\infty$ follows from tightness. Consider $q_1>q_2>\cdots$ with $q\to x$, then $F(q_{i+1})\leq F_\ast(q_i)$ for all $i$, and hence the right-continuity:
\[\lim_{i\to\infty}F(q_{i+1})\leq\lim_{i\to\infty}F_\ast(q_i)=F(x).\]
Also, by the choice of $q_i$,
\[\limsup_{k\to\infty}F_{n_k}(x)\leq\lim_{k\to\infty}F_{n_k}(q_i)=F_\ast(q_i)\]
holds for all $q_i$, hence $\limsup_{k\to\infty}F_{n_k}(x)\leq F(x)$ as $i\to\infty$. Reverse the inequality for limit infimum and obtain $F(x)\leq\liminf_{k\to\infty}F_{n_k}(x)$. Together $F(x)=\lim_{k\to\infty}F_{n_k}(x)$ as desired.\medbreak
Also by definition $\epsilon/\abs{c_n}\to0$ for any $\epsilon>0$. By tightness, for any $\delta>0$ take $K$ such that $\sup_n\mathbb{P}(\abs{X_n}\geq K)<\delta$, and take $N$ such that $\epsilon/\abs{c_n}>K$ for all $n>N$. Then
\begin{align*}
    &\mathbb{P}(\abs{X_n}\geq\epsilon/\abs{c_n})\leq\mathbb{P}(\abs{X_n}\geq K)\\
    \Rightarrow&\sup_{n\geq N}\mathbb{P}(\abs{X_n}\geq\epsilon/\abs{c_n})\leq\sup_{n\geq N}\mathbb{P}(\abs{X_n}\geq K)\leq\sup_n\mathbb{P}(\abs{X_n}\geq K)<\delta\\
    \Rightarrow&\sup_{n\geq N}\mathbb{P}(\abs{c_nX_n}\geq\epsilon)<\delta.
\end{align*}
And the desired result follows.\medbreak
Now suppose conversely that $X_n\xrightarrow{d}X$ and let $F_n$, $F$ be the distribution functions of $X_n$, $X$ respectively. We have
\begin{align*}
\mathbb{P}(\abs{X_n}>K)&=1-F_n(K)+F_n(-K)\\
&\leq(1-F(k))+F(-k)+\abs{F(k)-F_n(k)}+\abs{F_n(-k)-F_n(-k)}.
\end{align*}
Choose the appropriate $N$ and $K_0$, and the first two terms are bounded by $\epsilon/4$ by the fact that $F(k)\to1$ and $F(-k)\to0$ as $k\to\infty$; the second two are also bounded $\epsilon/4$ since $X_n\xrightarrow{d}X$. The first $N$ terms are bounded by some $K_1$, hence choose $K=\max{K_0,K_1}$ and $n>N$ and the tightness of $\{X_n\}$ follows.
\end{prop}

\begin{prop}
Assume $\mu_n\xrightarrow{weak}\mu$, then if $\{\mu_n\}$ is tight, for any $\epsilon>0$, we are able to find non-decreasing open sets $\{V_i\}$ such that $\cap_{i\geq1}V_i=\mathcal{S}$ and $\mu_n(V_i)=1-\epsilon$ for all $n$. Assume the contrary for contradiction, there exists for each $i$ some $n_i$ such that $\mu_{n_i}(V_i)\leq1-\epsilon$. Then since $\mu_k(V_i)\to1$ as $i\to\infty$, there cannot exists a $k$ such that $n_i=k$ for infinitely many $i$. Thus $n_i\to\infty$ as $i\to\infty$, and by Portmanteau lemma,
\[\mu(V_i)\leq\liminf_{j\to\infty}\mu_{n_j}(V_i)\leq\liminf_{j\to\infty}\mu_{n_j}(V_j)\leq1-\epsilon.\]
This is impossible, since $\mu(V_i)\to1$. Thus $\{\mu_n\}$ tight.\medbreak
To prove the converse, first we will construct a subsequence $\{\mu_{n_k}\}$ and from there some $\mu$, which we will show is an outer measure. By \textcolor{purple}{Prop xxx} the collection $\mathcal{F}$ of all $\mu$-measurable sets will be a $\sigma$-algebra on which $\mu$ is a measure. Then we will conclude the proof by showing that $\mathcal{B}(\mathcal{S})\subset\mathcal{F}$ and probability measure $\mu$ is indeed the weak limit.\medbreak
By tightness construct a sequence of non-decreasing compact sets $\{K_i\}$ such that $\mu_n(K_i)\geq1-1/i$ for all $n$. Since $\mathcal{S}$ is Polish, there exists a dense and countable $D\subset\mathcal{S}$. Let $\mathfrak{B}$ be the collection of all closed balls with centers at elements of $D$ and non-zero rational radius. Let $\mathfrak{C}$ be the collection of all finite unions of sets of the form $B\cap K_i$ where $B\in\mathfrak{B}$. Such $C$ is countable, hence there is a subsequence $\{\mu_{n_k}\}$ such that $\alpha(C)=\lim_{k\to\infty}\mu_{k_n}(C)$ exists for all $C\in\mathfrak{C}$. For any open set $V\subset\mathcal{S}$ define $\beta(V)=\sup\{\alpha(C):C\subset V\textrm{ and }C\in\mathfrak{C}\}$. Finally for any $A\subset\mathcal{S}$ define
\[\mu(A)=\inf\{\beta(V):A\subset V\textrm{ and }V\textrm{ is open}\}.\]
For any open $V_1,V_2\subset\mathcal{S}$ we have $\mu_{k_n}(V_1\cup V_2)\leq\mu_{k_n}(V_1)+\mu_{k_n}(V_2)$. By the construct of $\mathfrak{C}$ we are able to approach $V_1$ and $V_2$ with some $C,C_1,C_2\in\mathfrak{C}$ closely: 
\[\mu_{n_k}(C)=\mu_{n_k}(V_1\cup V_2)\leq\mu_{n_k}(V_1)+\mu_{n_k}(V_2)=\mu_{n_k}(C_1)+\mu_{n_k}(C_2),\]
for any $\epsilon>0$. Hence for open sets $\beta=\mu$ is finite subadditive, and then for any $A\subset\mathcal{S}$ the property also holds. The countable subadditivity follows by letting $\mu(V_i)\leq\mu(A_i)+2^{-i}\epsilon$. Therefore $\mu$ is indeed an outer measure.\medbreak
For open $V$ and closed $F$ subsets of $\mathcal{S}$, take $C_1\subset V\cap F^c$ and $C_2\subset V\cap C_1^c$ as subsets of $\mathfrak{C}$. Then since $C_1\cap C_2=\varnothing$ and $C_1\cup C_2\subset V$, we have $\beta(V)\geq\alpha(C_1\amalg C_2)=\alpha(C_1)+\alpha(C_2)$.
Taking supremum over $C_1$ and $C_2$ we have, as $V\cap C_1^c\supset V\cap F$,
\[\beta(V)\geq\alpha(V\cap F^c)+\alpha(V\cap F)=\beta(V\cap F^c)+\beta(V\cap F)\geq\mu(V\cap F^c)+\mu(V\cap F).\]\medbreak
Now for any $G\subset V$ we have $\beta(V)\geq\mu(V\cap F)+\mu(V\cap F^c)\geq\mu(G\cap F)+\mu(G\cap F^c)$.
Then $F\subset\mathcal{F}$ taking infimum over $V$. Hence the $\sigma$-algebra $\mathcal{F}$ of all $\mu$-measurable sets contains $\mathcal{B}(\mathcal{S})$.\medbreak
Finally, for any open $V\subset\mathcal{S}$ and $C\subset V$ for $C\in\mathfrak{C}$,
\[\alpha(C)=\lim_{k\to\infty}\mu_{n_k}(C)\leq\liminf_{k\to\infty}\mu_{n_k}(V).\]
Taking supremum over $C$, and by Portmanteau lemma $\mu_{n_k}\xrightarrow{weak}\mu$.
\end{prop}

\begin{prop}
We shall first prove a lemma regarding probability density function: if $X$ and $Y$ are independent and $Y$ has probability density function $g$, then $Z=X+Y$ has probability density function $h(z)=\E[g(z-X)]$.\medbreak
Indeed, $\mu_X\mu_Y$ is the law of $(X,Y)$ by independence, and apply Fubini-Tonelli Theorem twice:
\begin{align*}
F_Z(t)=\mathbb{P}(X+Y\leq t)&=\int\1_{\{X+Y\leq t\}}\,d(\mu_X\mu_Y)\\
&=\int\,d\mu_X\int\1_{\{Y\leq t+X\}}\,d\mu_Y\\
&=\int\,d\mu_X\int_{-\infty}^{t-x}g(y)\,dy\\
&=\int\,d\mu_X\int_{-\infty}^tg(z-x)\,dz=\int_{-\infty}^t\E(g(z-x))\,dz.
\end{align*}
Following the calculation of Normal distribution in the previous remark,
\[\int_{-\infty}^\infty e^{i(y-x)t-\theta t^2}\,dt=\sqrt{\frac{\pi}{\theta}}\int_{-\infty}^\infty\exp{\frac{i(y-x)}{\sqrt{2\theta}}s}\frac{1}{\sqrt{2\pi}}e^{-s^2/2}\,ds=\sqrt{\frac{\pi}{\theta}}\exp{\frac{-(y-x)^2}{4\theta}}.\]
Since $\phi_X(t)=\int_\mathbb{R}e^{ity}\,d\mu(y)$ where $\mu$ is the law of $X$, we apply Fubini-Tonelli Theorem,
\begin{align*}
f_\theta(X)=\frac{1}{2\pi}\int_{-\infty}^\infty e^{itX-\theta t^2}\phi_X(t)\,dt&=\frac{1}{2\pi}\int_{-\infty}^\infty\,d\mu(y)\int_{-\infty}^\infty e^{i(y-x)t-\theta t^2}\,dt\\&=\int_{-\infty}^\infty\frac{1}{\sqrt{4\pi\theta}}\exp{\frac{-(y-x)^2}{4\theta}}\,d\mu(y).
\end{align*}
By lemma and the previous remark $f_\theta(X)$ is the probability density function of $X+Y_\theta$ for some $Y_\theta\sim \mathcal{N}(0,2\theta)$. By Chebyshev's Inequality $Y_\theta\xrightarrow{\mathbb{P}}0$ as $\theta\to0$, hence $X+Y_\theta\xrightarrow{d}X$ by Slutsky's Theorem, and the result follows from definition.
\end{prop}

\begin{prop}
If $X_n\xrightarrow{d}X$ then by our previous decomposition $\phi_{X_n}(t)\to\phi_X(t)$ necessarily.\medbreak
On the other hand let $\phi_{X_n}(t)\to\phi_X(t)$, and assume for contradiction that there is a subsequence $X_{n_k}\not\xrightarrow{d}X$. For any $\epsilon>0$ choose small enough $a$ such that $2\abs{\phi_X(s)-1}\leq\epsilon$. Then by Dominated Convergence Theorem,
\[\lim_{n\to\infty}\int_{-a}^a\abs{1-\phi_{X_n}(s)}\,ds=\int_{-a}^a\abs{1-\phi_X(s)}\,ds\leq a\epsilon.\]
Let $at=2$, then by prop xxx $\limsup_{n\to\infty}\mathbb{P}(\abs{X_n}\geq t)\leq\epsilon$, which implies that $\{X_n\}$ is a tight family, as is $\{X_{n_k}\}$. By Helly's Selection Theorem we extract a subsequence $X_{n_k}\xrightarrow{d}X$, which contradicts the assumption. Hence $X_n\xrightarrow{d}X$.
\end{prop}

\begin{prop}
We shall only prove the case when $\mu=0$ and $\sigma=1$. The general case follows. By Lévy's Continuity Theorem we only need to prove that $\phi_{S_n}(t)\to e^{-t^2/2}$. We need two lemmas:\begin{itemize}
    \item By Lagrange remainder of Taylor expansion and
    \[\abs{e^{ix}-1-ix+\frac{x^2}{2}}\leq\abs{e^{ix}-1-ix}+\frac{x^2}{2}\leq\frac{x^2}{2}+\frac{x^2}{2}=x^2,\]
    we conclude that $\abs{e^{ix}-1-ix+\frac{x^2}{2}}\leq\min\{x^2,\abs{x}^3/6\}$.
    \item For $\abs{a_i}\leq1$ and $\abs{b_i}\leq1$,
    \begin{align*}
    \abs{\prod_{i=1}^na_i-\prod_{i=1}^nb_i}&\leq\sum_{i=1}^n\abs{a_1\cdots a_{i-1}b_i\cdots b_n-a_1\cdots a_ib_{i+1}\cdots b_n}\\
    &=\sum_{i=1}^n\abs{a_1\cdots a_{i-1}(b_i-a_i)b_{i+1}\cdots b_n}\leq\sum_{i=1}^n\abs{b_i-a_i}.
    \end{align*}
\end{itemize}
Now by independence and properties of characteristic function $\phi_{s_n}(t)=\prod_{i=1}^n\phi_{X_i}(t/\sqrt{n})=\phi_{X_1}^n(t/\sqrt{n})$. Choose large enough $n$ such that $t^2\leq2n$ and note that since $(1-\frac{t^2}{2n})^n\to e^{-t^2/2}$,
\begin{align*}
    \abs{\phi_{X_1}^n(t/\sqrt{n})-(1-\frac{t^2}{2n})^n}&\leq\sum_{i=1}^n\abs{\phi_{X_1}(t/\sqrt{n})-(1-\frac{t^2}{2n})}\\
    &=n\abs{\E\left(e^{itX_i/\sqrt{n}}-1-\frac{itX_1}{\sqrt{n}}+\frac{x^2t^2}{2n}\right)}\\
    &\leq n\E\left(\min\left\{t^2x^2,\frac{\abs{t}^3\abs{X_1}^3}{6\sqrt{n}}\right\}\right).
\end{align*}
The result then follows.
\end{prop}



\end{document}