\documentclass[hidelinks,11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amsfonts}
\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage[mathscr]{eucal}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage[OT1]{fontenc}
\usepackage{physics}
\usepackage{tikz-cd}
\usepackage{xpatch}
\usepackage{nicefrac}
\usepackage{mathtools}
\usepackage{environ}

\setlength\parindent{0pt}

\newtheoremstyle{dotless}{}{}{\itshape}{}{\bfseries}{}{ }{}

\newtheoremstyle{dotles}{}{}{\upshape}{}{\bfseries}{}{ }{}

\theoremstyle{definition}
\newtheorem*{defin}{DEF}

\theoremstyle{dotles}
\newtheorem{innercustomex}{EX}
\newenvironment{exercise}[1]
  {\renewcommand\theinnercustomex{#1}\innercustomex}
  {\endinnercustomex}

\theoremstyle{dotless}
\newtheorem{prop}{PROP}[section]
\newtheorem*{corollary}{Corollary}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\usepackage{hyperref}
\hypersetup{colorlinks=false}

\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\R}{\mathbb{R}}

\let\oldproof\proof
\renewcommand{\proof}{\color{blue}\oldproof}

%\NewEnviron{killcontents}{}\let\proof\killcontents\let\endproof\endkillcontents



\begin{document}

\section*{Preface}
This is the note for STAT 381-382-385 sequence at UChicago, which I took as a freshman and a sophomore in 2021. STAT 381 covers xxx, STAT 382 covers xxx, and STAT 385 covers xxx.
\tableofcontents

\section{Measure Theory}

\subsection{Measures}

\subsection{Integration}

\subsection{Measure on product spaces}

\section{Basic Probability}

\subsection{Basic notions}

\subsection{Convergence of random variables}

\bigbreak
\subsection{Laws of large numbers}

\begin{defin}
triangular array of random variables; truncation
\end{defin}

\begin{prop}[Weak Laws of Large Numbers]
Let $\{X_i\}$ be a sequence of random variables, $\{Y_{n,k}\}$ a triangular array of random variables with $1\leq k\leq n$, and $S_n=\sum_{i=1}^nX_i$.
\begin{itemize}
    \item \textup{($L^2$ Weak Law)} If $\mathbb{E}(X_iX_j)=\mathbb{E}(X_i)\mathbb{E}(X_j)=\mu^2$ and $\Var(X_i)\leq c<\infty$, then
    \[\frac{1}{n}S_n\xrightarrow{L^2}\mu\quad\textrm{and thus}\quad\frac{1}{n}S_n\xrightarrow{\mathbb{P}}\mu.\]
    \item If $a_n$ satisfies $a_n^{-2}\Var(S_n)\to0$, then $a_n^{-1}(S_n-\mathbb{E}(S_n))\to0$.
    \item \textup{(Triangular Array Weak Law)} Let $b_n\to\infty$ be a positive sequence and truncation $\overline{Y}_{n,k}=Y_{n,k}1_{\{\abs{Y_{n,k}}\leq b_n\}}$. If $\{Y_{n,k}\}$ is independent for every $n$ and satisfies
    \[\sum_{k=1}^n\mathbb{P}(\abs{Y_{n,k}>b_n})\to0\quad\textrm{and}\quad\frac{1}{b_n^2}\sum_{k=1}^n\mathbb{E}(\overline{Y}_{n,k}^2)\to0\]
    Then $b_n^{-1}(\sum_{i=1}^nY_{n,i}-\overline{\mu}_n)\xrightarrow{\mathbb{P}}0$ for $\overline{\mu}_n=\sum_{i=1}^n\mathbb{E}(\overline{Y}_{n,i})$.
    \item \textup{(Weak Law of Large Numbers)} Let truncation $\overline{X}_i=X_i1_{\{\abs{X_i}\leq n\}}$. If $\{X_i\}$ is independent and identically distributed with $n\mathbb{P}(\abs{X_i}>n)\to\infty$, then
    \[\frac{1}{n}S_n\xrightarrow{\mathbb{P}}\mathbb{E}(\overline{X}_i).\]
\end{itemize}
\end{prop}

example: coupon collector's problem

\begin{prop}[Strong Law of Large Numbers]
Let $\{X_i\}$ be a sequence of pairwise independent and identically distributed random variables with $\mathbb{E}(\abs{X_i})<\infty$, then for $\mathbb{E}(X_1)=\mu$,
\[\frac{1}{n}\sum_{i=1}^nX_i\xrightarrow{a.s.}\mu.\]
\end{prop}
\begin{proof}
The proof is due to Etemadi (1981). Assume for simplicity that $X_i\geq0$. Let $k_n=\left\lfloor{\alpha^n}\right\rfloor$ for some arbitrary $\alpha>1$. We will show that it suffices to show that $\frac{1}{k_n}\sum_{i=k_1}^{k_n}(Y_i-\E(Y_i))\to0$ as $n\to\infty$ for $Y_i=X_i\textbf{1}_{\{X_i<i\}}$.\smallbreak
Indeed, since
\[.\]
By Borel-Cantelli Lemma $\mathbb{P}(X_i\neq Y_i\textrm{ infinitely often})=0$. Hence $\sum_{i=1}^nX_i\xrightarrow{a.s.}\sum_{i=1}^nY_i$.On the other hand, since xxx, by Dominated Convergence Theorem,
\[,\]
which implies that $\frac{1}{n}\sum_{i=1}^n\E(Y_i)\xrightarrow{a.s.}\mu$. Hence our previous claim. Finally, for all $m$ such that $k_n\leq m<k_{n+1}$, the desired result follows from
\[.\]
Now We prove the deduced statement.
\end{proof}

\bigbreak
\subsection{Characteristic functions}

\begin{defin}
A sequence of random variables $\{X_n\}$ is a \textbf{tight family} if for any $\epsilon>0$ there exists some $k$ such that $\sup_n\mathbb{P}(\abs{X_n}\geq k)\leq\epsilon$.
\end{defin}

\begin{prop}
If a sequence of random variables $\{X_n\}$ converges to $X$ in distribution then $\{X_n\}$ is a tight family.
\end{prop}
\begin{proof}
r
\end{proof}

\begin{prop}[Helly's Selection Theorem]
Conversely, if $\{X_n\}$ is a tight family, then there is a subsequence $\{X_{n_k}\}$ that converges in distribution.
\end{prop}
\begin{proof}
Let $F_n$ be the distribution function of $X_n$. Since $F_n$ is bounded, there is a subsequence $F_{n_k}$ that converges pointwise $F_\ast(q)=\lim_{k\to\infty}F_{n_k}(q)$. Define $F(x)=\inf\{F_\ast(q):q>x\}$. We will show that $F(x)$ is a distribution function and $F(x)=\lim_{k\to\infty}F_{n_k}(x)$.\smallbreak
In order for $F(x)$ to be a distribution function, we only need to check the criterion in prop xxx. The monotonicity is inherited. $F(x)\to0$ as $x\to-\infty$ and $F(x)\to1$ as $x\to\infty$ follows from tightness. Consider $q_1>q_2>\cdots$ with $q\to x$, then $F(q_{i+1})\leq F_\ast(q_i)$ for all $i$, and hence the right-continuity:
\[\lim_{i\to\infty}F(q_{i+1})\leq\lim_{i\to\infty}F_\ast(q_i)=F(x).\]
Also, by the choice of $q_i$,
\[\limsup_{k\to\infty}F_{n_k}(x)\leq\lim_{k\to\infty}F_{n_k}(q_i)=F_\ast(q_i)\]
holds for all $q_i$, hence $\limsup_{k\to\infty}F_{n_k}(x)\leq F(x)$ as $i\to\infty$. Reverse the inequality for $\liminf$ and obtain $F(x)\leq\liminf_{k\to\infty}F_{n_k}(x)$. Together $F(x)=\lim_{k\to\infty}F_{n_k}(x)$ as desired.
\end{proof}

\bigbreak
\begin{defin}
The \textbf{characteristic function} of a random variable $X$ is $\phi_X(t):=\E(e^{itX})$.
\end{defin}

\begin{prop}
Some immediate properties of characteristic functions include
\begin{itemize}
    \item $\abs{\phi(t)}\leq\E\abs{e^{itX}}=1$,
    \item $\phi(t)$ is uniformly continuous: $\abs{\phi(t+h)-\phi(t)}\leq\E\abs{e^{ihX}-1}$,
    \item if $X$ and $Y$ are independent, then $\phi_{X+Y}=\phi_X\phi_Y$.
\end{itemize}
\end{prop}

\begin{remark}
The characteristic function of normal distribution $X\sim\mathcal{N}(0,1)$ is $\phi_X(t)=e^{-t^2/2}$. This can be verified
\end{remark}

\begin{prop}
For any random variable $X$ and $t>0$
\[\mathbb{P}(\abs{X}\geq t)\leq\frac{t}{2}\int_{-\frac{2}{t}}^{\frac{2}{t}}(1-\phi_X(s))\,ds.\]
\end{prop}

\begin{proof}
ee
\end{proof}

\begin{prop}[Inversion Formula]
For any random variable $X$ and $\theta>0$ define
\[f_\theta(X)=\frac{1}{2\pi}\int_{-\infty}^\infty e^{itX-\theta t^2}\phi_X(t)\,dt.\]
Then for any bounded continuous function $g:\R\to\R$,
\[\E[g(X)]=\lim_{\theta\to0}\int_{-\infty}^\infty g(X)f_\theta(X)\,dX.\]
\end{prop}
\begin{proof}
We shall first prove a lemma regarding probability density function: If $X$ and $Y$ are independent and $Y$ has probability density function $g$, then $Z=X+Y$ has probability density function $h(z)=\E[g(Z-X)]$.\smallbreak
Indeed,\smallbreak
Following the calculation of Normal distribution in the previous remark,
\[\int_{-\infty}^\infty e^{i(y-x)t-\theta t^2}\,dt=\sqrt{\frac{\pi}{\theta}}\int_{-\infty}^\infty\exp{\frac{i(y-x)}{\sqrt{2\theta}}s}\frac{1}{\sqrt{2\pi}}e^{-s^2/2}\,ds=\sqrt{\frac{\pi}{\theta}}\exp{\frac{-(y-x)^2}{4\theta}}.\]
Since $\phi_X(t)=\int_\mathbb{R}e^{ity}\,d\mu(y)$ where $\mu$ is the law of $X$, we apply Fubini-Tonelli Theorem,
\begin{align*}
f_\theta(X)=\frac{1}{2\pi}\int_{-\infty}^\infty e^{itX-\theta t^2}\phi_X(t)\,dt&=\frac{1}{2\pi}\int_{-\infty}^\infty\,d\mu(y)\int_{-\infty}^\infty e^{i(y-x)t-\theta t^2}\,dt\\&=\int_{-\infty}^\infty\frac{1}{\sqrt{4\pi\theta}}\exp{\frac{-(y-x)^2}{4\theta}}\,d\mu(y).
\end{align*}
By lemma and the previous remark $f_\theta(X)$ is the probability density function of $X+Y_\theta$ for $Y_\theta\sim \mathcal{N}(0,2\theta)$. By Chebyshev's Inequality $Y_\theta\xrightarrow{\mathbb{P}}0$ as $\theta\to0$, hence by Slutsky's Theorem $X+Y_\theta\xrightarrow{d}X$, and the result follows from definition.
\end{proof}

\begin{corollary}
Two random variables have the same law if and only if they have the same characteristic function.
\end{corollary}
\begin{proof}
If $X$ and $Y$ have the same law, then they have the same $k$-th moment $\E[X^k]=\E[Y^k]$ for all $k$. The characteristic function $\E[e^{iXt}]$ can be decomposed to sum of $k$th moments:
\begin{align*}
\E[e^{iXt}]&=\E(1+iXt-\frac{X^2t^2}{2!}+\cdots+\frac{i^nX^nt^n}{n!}+\cdots)\\&=1+it\E[X]-\frac{t^2\E[X^2]}{2!}+\cdots+\frac{t^n\E[X^n]}{n!}+\cdots.
\end{align*}
On the other hand, if $X$ and $Y$ have the same characteristic function, then by prop xxx and xxx they have the same law.
\end{proof}

\begin{corollary}
Suppose $\int_{-\infty}^\infty\abs{\phi_X(t)}\,dt<\infty$, then the probability density function of $X$ is given by
\[f(x)=\frac{1}{2\pi}\int_{-\infty}^\infty e^{-itx}\phi_X(t)\,dt.\]
\end{corollary}

\begin{prop}[LÃ©vy's Continuity Theorem]
A sequence of random variables $\{X_n\}$ converges to $X$ in distribution if and only if their characteristic functions $\{\phi_{X_n}\}$ converges to $\phi_X$ pointwise.
\end{prop}
\begin{proof}
If $X_n\xrightarrow{d}X$ then by our previous decomposition $\phi_{X_n}(t)\to\phi_X(t)$ necessarily.\smallbreak
On the other hand let $\phi_{X_n}(t)\to\phi_X(t)$, and assume for contradiction that there is a subsequence $X_{n_k}\not\xrightarrow{d}X$. For any $\epsilon>0$ choose small enough $a$ such that $2\abs{\phi_X(s)-1}\leq\epsilon$. Then by Dominated Convergence Theorem,
\[\lim_{n\to\infty}\int_{-a}^a\abs{1-\phi_{X_n}(s)}\,ds=\int_{-a}^a\abs{1-\phi_X(s)}\,ds\leq a\epsilon.\]
Let $at=2$, then by prop xxx $\limsup_{n\to\infty}\mathbb{P}(\abs{X_n}\geq t)\leq\epsilon$, which implies that $\{X_n\}$ is a tight family, as is $\{X_{n_k}\}$. By Helly's Selection Theorem we extract a subsequence $X_{n_k}\xrightarrow{d}X$, which contradicts the assumption. Hence $X_n\xrightarrow{d}X$.
\end{proof}

\begin{prop}
Let $\phi$ be a function and $t_1<t_2<\cdots<t_n$. Let $A$ be a matrix with entries $a_{ij}=\phi(t_i-t_j)$.\begin{itemize}
    \item \textup{(Bochner's Theorem)} If\begin{itemize}
        \item A is Hermitian: $A^*=\overline{A^T=A}$,
        \item A is positive semi-definite: for any $\forall v\in\mathbb{C}^n:v^TAv\geq0$,
        \item $\phi$ is continuous at 0 with $\phi(0)=1$,
    \end{itemize}
    Then $\phi$ is the characteristic function of some random variable.
    \item Conversely if $\phi$ is a characteristic function of some random variable, then it satisfies those properties.
\end{itemize}
\end{prop}

\begin{prop}[Central Limit Theorem, Classical]
Let $\{X_i\}_{i\geq1}$ be a sequence of independent and identically ditributed random variables with $\E(X_1)=\mu$ and $\Var(E_1)=\sigma^2<\infty$. Then
\[\frac{\sum_{i=1}^nX^i-\mu}{\sigma\sqrt{n}}\xrightarrow{d}\mathcal{N}(0,1).\]
\end{prop}

\begin{prop}[Central Limit Theorem]

\end{prop}

\begin{corollary}[Central Limit Theorem]

\end{corollary}



\end{document}