\documentclass[hidelinks,11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amsfonts}
\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage[mathscr]{eucal}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage[OT1]{fontenc}
\usepackage{physics}
\usepackage{tikz-cd}
\usepackage{xpatch}
\usepackage{nicefrac}
\usepackage{mathtools}
\usepackage{environ}

\setlength\parindent{0pt}

\newtheoremstyle{dotless}{}{}{\itshape}{}{\bfseries}{}{ }{}

\newtheoremstyle{dotles}{}{}{\upshape}{}{\bfseries}{}{ }{}

\theoremstyle{definition}
\newtheorem*{defin}{DEF}

\theoremstyle{dotles}
\newtheorem{innercustomex}{EX}
\newenvironment{exercise}[1]
  {\renewcommand\theinnercustomex{#1}\innercustomex}
  {\endinnercustomex}

\theoremstyle{dotless}
\newtheorem{prop}{PROP}[section]
\newtheorem*{corollary}{Corollary}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\usepackage{hyperref}
\hypersetup{colorlinks=false}

\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\R}{\mathbb{R}}

\let\oldproof\proof
\renewcommand{\proof}{\color{blue}\oldproof}

%\NewEnviron{killcontents}{}\let\proof\killcontents\let\endproof\endkillcontents



\begin{document}

\section*{Preface}
This is the note for STAT 381-382-385 sequence at UChicago, which I took as a freshman and a sophomore in 2021. STAT 381 covers xxx, STAT 382 covers xxx, and STAT 385 covers xxx.
\tableofcontents

\section{Measure Theory}

\subsection{Measures}

\subsection{Integration}

\subsection{Measure on product spaces}

\section{Basic Probability}

\subsection{Basic notions}

\subsection{Convergence of random variables}

\bigbreak
\subsection{Laws of large numbers}

\begin{defin}
triangular array of random variables; truncation
\end{defin}

\begin{prop}[Weak Laws of Large Numbers]
Let $\{X_i\}$ be a sequence of random variables, $\{Y_{n,k}\}$ a triangular array of random variables with $1\leq k\leq n$, and $S_n=\sum_{i=1}^nX_i$.
\begin{itemize}
    \item \textup{($L^2$ Weak Law)} If $\mathbb{E}(X_iX_j)=\mathbb{E}(X_i)\mathbb{E}(X_j)=\mu^2$ and $\Var(X_i)\leq c<\infty$, then
    \[\frac{1}{n}S_n\xrightarrow{L^2}\mu\quad\textrm{and thus}\quad\frac{1}{n}S_n\xrightarrow{\mathbb{P}}\mu.\]
    \item If $a_n$ satisfies $a_n^{-2}\Var(S_n)\to0$, then $a_n^{-1}(S_n-\mathbb{E}(S_n))\to0$.
    \item \textup{(Triangular Array Weak Law)} Let $b_n\to\infty$ be a positive sequence and truncation $\overline{Y}_{n,k}=Y_{n,k}1_{\{\abs{Y_{n,k}}\leq b_n\}}$. If $\{Y_{n,k}\}$ is independent for every $n$ and satisfies
    \[\sum_{k=1}^n\mathbb{P}(\abs{Y_{n,k}>b_n})\to0\quad\textrm{and}\quad\frac{1}{b_n^2}\sum_{k=1}^n\mathbb{E}(\overline{Y}_{n,k}^2)\to0\]
    Then $b_n^{-1}(\sum_{i=1}^nY_{n,i}-\overline{\mu}_n)\xrightarrow{\mathbb{P}}0$ for $\overline{\mu}_n=\sum_{i=1}^n\mathbb{E}(\overline{Y}_{n,i})$.
    \item \textup{(Weak Law of Large Numbers)} Let truncation $\overline{X}_i=X_i1_{\{\abs{X_i}\leq n\}}$. If $\{X_i\}$ is independent and identically distributed with $n\mathbb{P}(\abs{X_i}>n)\to\infty$, then
    \[\frac{1}{n}S_n\xrightarrow{\mathbb{P}}\mathbb{E}(\overline{X}_i).\]
\end{itemize}
\end{prop}

example: coupon collector's problem

\begin{prop}[Strong Law of Large Numbers]
Let $\{X_i\}$ be a sequence of pairwise independent and identically distributed random variables with $\mathbb{E}(\abs{X_i})<\infty$, then for $\mathbb{E}(X_1)=\mu$,
\[\frac{1}{n}\sum_{i=1}^nX_i\xrightarrow{a.s.}\mu.\]
\end{prop}
\begin{proof}
The proof is due to Etemadi (1981). As a lemma, $y\sum_{k>y}k^{-2}\leq2$ for $y\geq0$.\smallbreak
Assume for simplicity that $X_i\geq0$. Let $k_n=\left\lfloor{\alpha^n}\right\rfloor$ for some arbitrary $\alpha>1$, $Y_i=X_i\textbf{1}_{\{X_i<i\}}$, and $Z_n=\frac{1}{n}\sum_{i=1}^n(Y_i-\E(Y_i))$.\smallbreak
By Chebyshev's inequality,  Fubini-Tonelli Theorem, and the fact that $\alpha^n\leq2\left\lfloor{\alpha^n}\right\rfloor$,
\begin{align*}
\sum_{n=1}^\infty\mathbb{P}(\abs{Z_{k_n}}>\epsilon)&\leq\epsilon^{-2}\sum_{n=1}^\infty k_n^{-2}\sum_{i=k_1}^{k_n}\Var(Y_i)\\
&=\epsilon^{-2}\sum_{i=1}^\infty\Var(Y_i)\sum_{n:k_n\geq i}k_n^{-2}\leq4(1-\alpha^{-2})^{-1}\epsilon^{-2}\sum_{i=1}^\infty i^{-2}\Var(Y_i).
\end{align*}
Since by Fubini-Tonelli Theorem, the lemma, and the assumption $\E(\abs{X_i})<\infty$
\[\sum_{i=1}^\infty i^{-2}\Var(Y_i)\leq\sum_{i=1}^\infty i^{-2}\E(Y_i^2)\leq2\int_0^\infty\,dy\left\{\sum_{i=1}^\infty i^{-2}\textbf{1}_{\{y<k\}}\right\}y\mathbb{P}(\abs{X_1}>y)\leq4\E(\abs{X_i})<\infty,\]
we conclude that $\mathbb{P}(\abs{Z_{n_k}}>\epsilon)\to0$, or $k_n^{-1}\sum_{i=k_1}^{k_n}Y_i\to k_n^{-1}\sum_{i=k_1}^{k_n}\E(Y_i)$. To deal with middle values note that for $k_n<m<k_{n+1}$
\[k_{n+1}^{-1}\sum_{i=1}^{k_n}Y_i\leq m^{-1}\sum_{i=1}^mY_i\leq k_n^{-1}\sum_{i=k_1}^{k_{n+1}}Y_i.\]
Hence $n^{-1}\sum_{i=1}^nY_i\to n^{-1}\sum_{i=1}^n\E(Y_i)$.\smallbreak
Since $\sum_{i=1}^\infty\mathbb{P}(X_i\neq Y_i)=\sum_{i=1}^\infty\mathbb{P}(X_1\geq i)\leq\E(X_1)<\infty$, by Borel Cantelli Lemma $\mathbb{P}(X_i\neq Y_i,\textrm{ infinitely often})=0$, which implies that $n^{-1}\sum_{i=1}^nX_i\xrightarrow{a.s.}n^{-1}\sum_{i=1}^nY_i$.\smallbreak
Lastly by Dominated Convergence Theorem $\E(Y_i)-\mu=\E(X_1\textbf{1}_{\{X_i\geq i\}})\to0$, which implies that $n^{-1}\sum_{i=1}^n\E(Y_i)\to\mu$. Combining these result and the statement follows:
\[n^{-1}\sum_{i=1}^nX_i\xrightarrow{a.s.}n^{-1}\sum_{i=1}^nY_i\to n^{-1}\sum_{i=1}^n\E(Y_i)\to\mu\]
\end{proof}

\bigbreak
\subsection{Characteristic functions}

\begin{defin}
A sequence of random variables $\{X_n\}$ is a \textbf{tight family} if for any $\epsilon>0$ there exists some $k$ such that $\sup_n\mathbb{P}(\abs{X_n}\geq k)\leq\epsilon$.
\end{defin}

\begin{prop}
If a sequence of random variables $\{X_n\}$ converges to $X$ in distribution then $\{X_n\}$ is a tight family.
\end{prop}
\begin{proof}
r
\end{proof}

\begin{prop}[Helly's Selection Theorem]
Conversely, if $\{X_n\}$ is a tight family, then there is a subsequence $\{X_{n_k}\}$ that converges in distribution.
\end{prop}
\begin{proof}
Let $F_n$ be the distribution function of $X_n$. Since $F_n$ is bounded, there is a subsequence $F_{n_k}$ that converges pointwise $F_\ast(q)=\lim_{k\to\infty}F_{n_k}(q)$. Define $F(x)=\inf\{F_\ast(q):q>x\}$. We will show that $F(x)$ is a distribution function and $F(x)=\lim_{k\to\infty}F_{n_k}(x)$.\smallbreak
In order for $F(x)$ to be a distribution function, we only need to check the criterion in prop xxx. The monotonicity is inherited. $F(x)\to0$ as $x\to-\infty$ and $F(x)\to1$ as $x\to\infty$ follows from tightness. Consider $q_1>q_2>\cdots$ with $q\to x$, then $F(q_{i+1})\leq F_\ast(q_i)$ for all $i$, and hence the right-continuity:
\[\lim_{i\to\infty}F(q_{i+1})\leq\lim_{i\to\infty}F_\ast(q_i)=F(x).\]
Also, by the choice of $q_i$,
\[\limsup_{k\to\infty}F_{n_k}(x)\leq\lim_{k\to\infty}F_{n_k}(q_i)=F_\ast(q_i)\]
holds for all $q_i$, hence $\limsup_{k\to\infty}F_{n_k}(x)\leq F(x)$ as $i\to\infty$. Reverse the inequality for $\liminf$ and obtain $F(x)\leq\liminf_{k\to\infty}F_{n_k}(x)$. Together $F(x)=\lim_{k\to\infty}F_{n_k}(x)$ as desired.
\end{proof}

\bigbreak
\begin{defin}
The \textbf{characteristic function} of a random variable $X$ is $\phi_X(t):=\E(e^{itX})$.
\end{defin}

\begin{prop}
Some immediate properties of characteristic functions include
\begin{itemize}
    \item $\abs{\phi(t)}\leq\E\abs{e^{itX}}=1$,
    \item $\phi(t)$ is uniformly continuous: $\abs{\phi(t+h)-\phi(t)}\leq\E\abs{e^{ihX}-1}$,
    \item if $X$ and $Y$ are independent, then $\phi_{X+Y}=\phi_X\phi_Y$.
\end{itemize}
\end{prop}

\begin{remark}
The characteristic function of normal distribution $X\sim\mathcal{N}(0,1)$ is $\phi_X(t)=e^{-t^2/2}$. This can be verified
\end{remark}

\begin{prop}
For any random variable $X$ and $t>0$
\[\mathbb{P}(\abs{X}\geq t)\leq\frac{t}{2}\int_{-\frac{2}{t}}^{\frac{2}{t}}(1-\phi_X(s))\,ds.\]
\end{prop}

\begin{proof}
ee
\end{proof}

\begin{prop}[Inversion Formula]
For any random variable $X$ and $\theta>0$ define
\[f_\theta(X)=\frac{1}{2\pi}\int_{-\infty}^\infty e^{itX-\theta t^2}\phi_X(t)\,dt.\]
Then for any bounded continuous function $g:\R\to\R$,
\[\E[g(X)]=\lim_{\theta\to0}\int_{-\infty}^\infty g(X)f_\theta(X)\,dX.\]
\end{prop}
\begin{proof}
We shall first prove a lemma regarding probability density function: If $X$ and $Y$ are independent and $Y$ has probability density function $g$, then $Z=X+Y$ has probability density function $h(z)=\E[g(Z-X)]$.\smallbreak
Indeed,\smallbreak
Following the calculation of Normal distribution in the previous remark,
\[\int_{-\infty}^\infty e^{i(y-x)t-\theta t^2}\,dt=\sqrt{\frac{\pi}{\theta}}\int_{-\infty}^\infty\exp{\frac{i(y-x)}{\sqrt{2\theta}}s}\frac{1}{\sqrt{2\pi}}e^{-s^2/2}\,ds=\sqrt{\frac{\pi}{\theta}}\exp{\frac{-(y-x)^2}{4\theta}}.\]
Since $\phi_X(t)=\int_\mathbb{R}e^{ity}\,d\mu(y)$ where $\mu$ is the law of $X$, we apply Fubini-Tonelli Theorem,
\begin{align*}
f_\theta(X)=\frac{1}{2\pi}\int_{-\infty}^\infty e^{itX-\theta t^2}\phi_X(t)\,dt&=\frac{1}{2\pi}\int_{-\infty}^\infty\,d\mu(y)\int_{-\infty}^\infty e^{i(y-x)t-\theta t^2}\,dt\\&=\int_{-\infty}^\infty\frac{1}{\sqrt{4\pi\theta}}\exp{\frac{-(y-x)^2}{4\theta}}\,d\mu(y).
\end{align*}
By lemma and the previous remark $f_\theta(X)$ is the probability density function of $X+Y_\theta$ for $Y_\theta\sim \mathcal{N}(0,2\theta)$. By Chebyshev's Inequality $Y_\theta\xrightarrow{\mathbb{P}}0$ as $\theta\to0$, hence by Slutsky's Theorem $X+Y_\theta\xrightarrow{d}X$, and the result follows from definition.
\end{proof}

\begin{corollary}
Two random variables have the same law if and only if they have the same characteristic function.
\end{corollary}
\begin{proof}
If $X$ and $Y$ have the same law, then they have the same $k$-th moment $\E[X^k]=\E[Y^k]$ for all $k$. The characteristic function $\E[e^{iXt}]$ can be decomposed to sum of $k$th moments:
\begin{align*}
\E[e^{iXt}]&=\E(1+iXt-\frac{X^2t^2}{2!}+\cdots+\frac{i^nX^nt^n}{n!}+\cdots)\\&=1+it\E[X]-\frac{t^2\E[X^2]}{2!}+\cdots+\frac{t^n\E[X^n]}{n!}+\cdots.
\end{align*}
On the other hand, if $X$ and $Y$ have the same characteristic function, then by prop xxx and xxx they have the same law.
\end{proof}

\begin{corollary}
Suppose $\int_{-\infty}^\infty\abs{\phi_X(t)}\,dt<\infty$, then the probability density function of $X$ is given by
\[f(x)=\frac{1}{2\pi}\int_{-\infty}^\infty e^{-itx}\phi_X(t)\,dt.\]
\end{corollary}

\begin{prop}[Lévy's Continuity Theorem]
A sequence of random variables $\{X_n\}$ converges to $X$ in distribution if and only if their characteristic functions $\{\phi_{X_n}\}$ converges to $\phi_X$ pointwise.
\end{prop}
\begin{proof}
If $X_n\xrightarrow{d}X$ then by our previous decomposition $\phi_{X_n}(t)\to\phi_X(t)$ necessarily.\smallbreak
On the other hand let $\phi_{X_n}(t)\to\phi_X(t)$, and assume for contradiction that there is a subsequence $X_{n_k}\not\xrightarrow{d}X$. For any $\epsilon>0$ choose small enough $a$ such that $2\abs{\phi_X(s)-1}\leq\epsilon$. Then by Dominated Convergence Theorem,
\[\lim_{n\to\infty}\int_{-a}^a\abs{1-\phi_{X_n}(s)}\,ds=\int_{-a}^a\abs{1-\phi_X(s)}\,ds\leq a\epsilon.\]
Let $at=2$, then by prop xxx $\limsup_{n\to\infty}\mathbb{P}(\abs{X_n}\geq t)\leq\epsilon$, which implies that $\{X_n\}$ is a tight family, as is $\{X_{n_k}\}$. By Helly's Selection Theorem we extract a subsequence $X_{n_k}\xrightarrow{d}X$, which contradicts the assumption. Hence $X_n\xrightarrow{d}X$.
\end{proof}

\begin{prop}
Let $\phi$ be a function and $t_1<t_2<\cdots<t_n$. Let $A_{ij}=\phi(t_i-t_j)$ be a $n\times n$ matrix.\begin{itemize}
    \item \textup{(Bochner's Theorem)} Then $\phi$ is the characteristic function of some random variable if\begin{itemize}
        \item A is Hermitian: $A^*=\overline{A^T=A}$,
        \item A is positive semi-definite: for any $\forall v\in\mathbb{C}^n:v^TAv\geq0$,
        \item $\phi$ is continuous at 0 with $\phi(0)=1$,
    \end{itemize}
    \item Conversely if $\phi$ is a characteristic function of some random variable, then it satisfies those properties.
\end{itemize}
\end{prop}

\begin{prop}[Central Limit Theorem]
Let $\{X_i\}_{i\geq1}$ be a sequence of independent and identically ditributed random variables with $\E(X_1)=\mu$ and $\Var(E_1)=\sigma^2<\infty$. Then
\[S_n=\frac{\sum_{i=1}^nX_i-\mu}{\sigma\sqrt{n}}\xrightarrow{d}\mathcal{N}(0,1).\]
\end{prop}
\begin{proof}
We shall only prove the case when $\mu=0$ and $\sigma=1$. The general case follows. By Lévy's Continuity Theorem we only need to prove that $\phi_{S_n}(t)\to e^{-t^2/2}$. We need two lemmas:\begin{itemize}
    \item By Lagrange remainder of Taylor expansion and
    \[\abs{e^{ix}-1-ix+\frac{x^2}{2}}\leq\abs{e^{ix}-1-ix}+\frac{x^2}{2}\leq\frac{x^2}{2}+\frac{x^2}{2}=x^2,\]
    we conclude that $\abs{e^{ix}-1-ix+\frac{x^2}{2}}\leq\min\{x^2,\abs{x}^3/6\}$.
    \item For $\abs{a_i}\leq1$ and $\abs{b_i}\leq1$,
    \begin{align*}
    \abs{\prod_{i=1}^na_i-\prod_{i=1}^nb_i}&\leq\sum_{i=1}^n\abs{a_1\cdots a_{i-1}b_i\cdots b_n-a_1\cdots a_ib_{i+1}\cdots b_n}\\
    &=\sum_{i=1}^n\abs{a_1\cdots a_{i-1}(b_i-a_i)b_{i+1}\cdots b_n}\leq\sum_{i=1}^n\abs{b_i-a_i}.
    \end{align*}
\end{itemize}
Now by independence and properties of characteristic function $\phi_{s_n}(t)=\prod_{i=1}^n\phi_{X_i}(t/\sqrt{n})=\phi_{X_1}^n(t/\sqrt{n})$. Choose large enough $n$ such that $t^2\leq2n$ and note that since $(1-\frac{t^2}{2n})^n\to e^{-t^2/2}$,
\begin{align*}
    \abs{\phi_{X_1}^n(t/\sqrt{n})-(1-\frac{t^2}{2n})^n}&\leq\sum_{i=1}^n\abs{\phi_{X_1}(t/\sqrt{n})-(1-\frac{t^2}{2n})}\\
    &=n\abs{\E\left(e^{itX_i/\sqrt{n}}-1-\frac{itX_1}{\sqrt{n}}+\frac{x^2t^2}{2n}\right)}\\
    &\leq n\E\left(\min\left\{t^2x^2,\frac{\abs{t}^3\abs{X_1}^3}{6\sqrt{n}}\right\}\right).
\end{align*}
The result then follows.
\end{proof}

\begin{remark}
The above Central Limit Theorem is the classical result. A stronger one that reduces the identical distribution condition is Lindeberg-Feller Central Limit Theorem:\smallbreak
\textit{Let $\{k_n\}_{n\geq1}$ be a sequence of positive integers with $k_n\to\infty$. Let $\{X_{n,i}\}_{1\leq i\leq k_n}$ be a collection of independent random variables and denote $\mu_{n,i}=\E(X_{n,i})$, $\sigma_{n,i}^2=\Var(X_{n,i})$, and $S_n^2=\sum_{i=1}^{k_n}\sigma_{n,i}^2$. If $\{X_{n,i}\}$ satisfies Lindeberg condition:}
\[\forall\epsilon>0:\frac{1}{S_n^2}\sum_{i=1}^{k_n}\E\left[(X_{n,i}-\mu_{n,i})^2\textbf{1}_{\{\abs{X_{n,i}-\mu_{n,i}}\geq\epsilon S_n\}}\right]\to0.\]
\textit{Then}
\[\frac{1}{S_n}\sum_{i=1}^{k_n}(X_{n,i}-\mu_{n,i})\xrightarrow{d}\mathcal{N}(0,1).\]
As a corollary to the Lindeberg-Feller Central Limit Theorem is Lyapunov Central Limit Theorem:\smallbreak
\textit{Let $\{X_i\}_{i\geq1}$ be a sequence of independent random variables with $\E(X_i)=\mu_i$, $\sigma_i^2=\Var(X_i)<\infty$ and $S_n^2=\sum_{i=1}^n\sigma_i^2$. If $\{X_i\}$ satisfies Lyapunov condition:}
\[\exists\delta>0:\frac{1}{S_n^{2+\delta}}\sum_{i=1}^n\E\left[\abs{X_i-\mu_i}^{2+\delta}\right]\to0.\]
\textit{Then}
\[\frac{1}{S_n}\sum_{i=1}^n(X_i-\mu_i)\xrightarrow{d}\mathcal{N}(0,1).\]
\end{remark}

\bigbreak
\subsection{Probability theory on seperable metric spaces}
\begin{defin}
If we don't limit ourselves to $\mathbb{R}$, we have\begin{itemize}
    \item \textbf{random variable}: measurable $X:(\Omega,\mathcal{F},\mathbb{P})\to\mathbb{R}^d$,
    \item \textbf{law} of $X$: $\mu_X(A)=\mathbb{P}(X\in A)\quad\forall A\in\mathcal{B}(\mathbb{R}^d)$,
    \item \textbf{distribution function} of $X$: $F_X(t_1,\cdots,t_d)=\mathbb{P}(X_1\leq t_1\textrm{ and }\cdots\textrm{ and }X_d\leq t_d)$,
    \item \textbf{probability density function} of $X$: $f:\mathbb{R}^d\to[0,\infty]$ such that
    \[\mathbb{P}(X\in A)=\int_Af(x_1,\cdots,x_d)\,dx_1\cdots dx_d\quad A\in\mathcal{B}(\mathbb{R}^d)\].
\end{itemize}
Concepts are associated with product measure, and related results still holds in the multivariable sense.
\end{defin}


\end{document}