\documentclass[hidelinks,11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amsfonts}
\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage[mathscr]{eucal}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage[OT1]{fontenc}
\usepackage{physics}
\usepackage{tikz-cd}
\usepackage{xpatch}
\usepackage{nicefrac}
\usepackage{mathtools}
\usepackage{environ}

\setlength\parindent{0pt}

\newtheoremstyle{dotless}{}{}{\itshape}{}{\bfseries}{}{ }{}

\theoremstyle{definition}
\newtheorem*{defin}{DEF}

\theoremstyle{dotless}
\newtheorem{prop}{PROP}[section]
\newtheorem*{corollary}{Corollary}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\usepackage{hyperref}
\hypersetup{colorlinks=true,linkcolor=magenta}

\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\R}{\mathbb{R}}
\DeclareMathOperator{\1}{\mathbf{1}}

\let\oldproof\proof
\renewcommand{\proof}{\color{blue}\oldproof}

%\NewEnviron{killcontents}{}\let\proof\killcontents\let\endproof\endkillcontents



\begin{document}
\begin{center}
{\Large\textbf STAT 381-383-385 (2021)\hspace{0.1cm} Theorem Page}\medbreak
\large{Alex Sheng}
\end{center}

\vspace{0.2 cm}
\tableofcontents

\newpage
\section{Measure Theory}

\subsection{Measures}

\begin{prop}\label{Prop 1.1}
The following two properties for a finitely additive measure $\mu$ are equivalent:\begin{itemize}
    \item $\mu$ is downward continuous: if $\{E_i\}_{i\geq1}$ is a decreasing sequence of sets such that $\mu(E_k)<\infty$ for some $k$, then $\mu(E_i)\to\mu(\cap_{i\geq1}E_i).$
    \item $\mu$ is $\sigma$-additive.
\end{itemize}
\end{prop}

\begin{prop}\label{Prop 1.2}
Let $\mu$ be an outer measure on $\Omega$. Let $\mathcal{F}$ be the collection of all $\mu$-measurable subsets of $\Omega$. Then $\mathcal{F}$ is a $\sigma$-algebra and $\mu$ is a measure on $\mathcal{F}$.
\end{prop}

\begin{remark}
Hands on construction of Jordan measure and Lebesgue measure.\smallbreak
A subset of $\mathbb{R}^d$ is elementary if it is a finite union of finite boxes. Jordan measure $\mu$ of elementary set $E$ is simply the volume of $E$. Jordan measure of arbitrary $A\subset\mathbb{R}^d$ is defined by
\[\mu(A)=\inf\{\mu(E):A\subset E\textrm{ and }E\textrm{ is elementary}\}.\]
A set $A\subset\mathbb{R}^d$ is called Jordan measurable if for any $\epsilon>0$ there exists elementary sets $E$ and $F$ such that $E\subset A\subset F$ and $\mu(F\setminus E)<\epsilon$. Then the collection of all Jordan measurable sets is a Boolean algebra and $\mu$ is a finitely additive measure defined on it.\medbreak
For any $A\subset\mathbb{R}^d$ define
\[\mu^\ast(A)=\inf\left\{\sum_{n\geq1}\mu(B_n):E\subset\bigcup_{i\geq1}B_i\textrm{ and }B_n\textrm{ are boxes in }\mathbb{R}^d\right\}.\]
Then $\mu^\ast$ is an outer measure. A subset $A\subset\mathbb{R}^d$ is called Lebesgue measurable if for any $\epsilon>0$ there exists a countable union of boxes $C=\cup_{i\geq1}B_i$ such that $E\subset C$ and $\mu^\ast(C\setminus E)<\epsilon$. Without invoking \hyperref[Prop 1.1]{Prop 1.2}, we will show that\begin{itemize}
    \item $\mu^*$ extends $\mu$: $\mu^\ast(E)=\mu(E)$ for all Jordan measurable sets $E$,
    \item the collection of all Lebesgue measurable sets $\mathcal{L}$ is a $\sigma$-algebra, and
    \item $\mu^\ast|_\mathcal{L}$ is a measure.
\end{itemize}
Such extension $\mu$ is the Lebesgue measure on $\mathbb{R}^d$. $\mu^\ast$ is the Lebesgue outer measure.\smallbreak
We first show that $\mu$ is downward continuous. Indeed, let $\{E_k\}_{k\geq1}$ be a decreasing sequence of elementary sets and $\cap_{k\geq1}E_k=\varnothing$. Shrink a bit each side of $E_k$ and find a closed elementary $F_k\subset E_k$ such that $\mu(E_k\setminus E_k)\leq2^{-k-1}\epsilon$ for some $\epsilon>0$. Then
\[\mu(E_k)-\mu(\bigcap_{i=1}^kF_i)\leq\mu(E_k\setminus\bigcap_{i=1}^kF_i)=\mu(\bigcap_{i=1}^k(E_k\setminus F_i))\leq\sum_{i=1}^k\mu(E_i\setminus F_i)\leq\frac{\epsilon}{2}.\]
Since $\cap_{i\geq1}F_i=\varnothing$, we have $\lim_{k\to\infty}\mu(E_k)=0$ as desired.\medbreak
Now we will prove that $\mu^\ast(A)=\mu(A)$ for any Jordan measurable set $A$. Since $\mu^*$ takes infimum over countable cover of boxes and $\mu$ takes infimum over finite union of boxes, $\mu^\ast(A)\leq\mu(A)$. For the reverse inequality, first let $A$ be elementary, then there exists boxes $\{B_i\}_{i\geq1}$ such that $A\subset\cup_{i\geq1}B_i$ and $\sum_{i\geq1}\mu(B_i)\leq\mu^\ast(A)+\epsilon$ for any $\epsilon>0$. Let $E_n=A\setminus\cup_{i=1}^nB_i$ which is elementary and $\cap_{i\geq1}E_i=\varnothing$. By downward continuity of $\mu$,
\[\mu(A)\leq\mu(E_n)+\mu(\bigcup_{i=1}^nB_i)\leq\mu(E_n)+\sum_{i=1}^n\mu(B_i)\leq\mu(E_n)+\mu^\ast(A)+\epsilon.\]
Taking $n\to\infty$ the result follows. Now for arbitrary Jordan measurable $A$ we have elementary $E$ such that $E\subset A$ and $\mu(A)\leq\mu(E)+\epsilon=\mu^\ast(E)+\epsilon\leq\mu^\ast(A)+\epsilon$. Hence $\mu^\ast(A)=\mu(A)$.\medbreak
Trivially $\varnothing\in\mathcal{L}$. We only need to check for complement-stability and countable $\cup$-stability.\begin{itemize}
    \item Let $A=\cup_{i\geq1}A_i$ with $A_i\in\mathcal{L}$ for all $i$. For any $\epsilon>0$ let $C_i$ be a countable union of boxes with $A_i\subset C_i$ and $\mu^\ast(C_i\setminus A_i)\leq2^{-i}\epsilon$ for all $i$.
    \item r
\end{itemize}

\textcolor{blue}{\medbreak To prove that $\mathcal{L}$ is a $\sigma$-algebra prove first for countable union. Then for complement prove that Lebesgue-outer null sets are Lebesgue measurable.\smallbreak
For the last part prove first that any countable intersection of elementary sets are Lebesgue measurable. Then prove that bounded Lebesgue measurable sets can be approximated above by countable intersections of open sets (which are Lebesgue measurable). Lastly prove the finite additivity for countable intersections of elementary sets, which can be extended to the countable additivity on Lebesgue measurable sets.}
\end{remark}

\begin{prop}\textup{\textbf{(Dynkin's $\pi$-$\lambda$ Theorem)}} Let $\Pi$ be a $\pi$-system and $\Lambda$ a $\lambda$-system. If $\Pi\subset\Lambda$ then $\sigma(\Pi)\subset\Lambda$.
\end{prop}

\begin{prop}\textup{\textbf{(Carathéodory's Extension Theorem)}} Every pre-measure on a Boolean algebra $\mathfrak{B}$ can be extended to a measure on $\sigma(\mathfrak{B})$. The extension is unique if the pre-measure is $\sigma$-finite.
\end{prop}

\begin{prop}
properties of measurable functions
\end{prop}

\subsection{Integration}

\begin{prop}
Non-negative measurable functions can be pointwise approximated by a non-decreasing sequence of simple functions.
\end{prop}

\begin{prop}
properties of integration
\end{prop}

\begin{prop}
convergence theorems
\end{prop}

\subsection{Measure and integration on product spaces}

\begin{prop}
measurable functions on product space
\end{prop}

\begin{prop}
product measure
\end{prop}

\begin{prop}\textup{\textbf{(Fubini-Tonelli Theorem)}} Lr
\end{prop}

\newpage
\section{Basic Proabbility}

\subsection{Basic notions}

\begin{prop}\label{Prop 2.1}
If $F$ is the distribution function of random variable $X$, then $F$ is:\begin{itemize}
    \item non-decreasing,
    \item right-continuous: $\lim_{\Delta t\to0}F(\Delta t+t)=F(t)$,
    \item $\lim_{t\to-\infty}F(t)=0$, and $\lim_{t\to\infty}F(t)=1$.
\end{itemize}
Moreover, $F$ determines the law $\mu_X$ of $X$ uniquely. Conversely if $F:\mathbb{R}\to[0,1]$ satisfies the listed properties, then there is a probability space $(\Omega, \mathcal{F},\mathbb{P})$ and a random variable $X$ such that $F$ is the distribution function of $X$.
\end{prop}

\begin{prop}\textup{\textbf{(Jensen's Inequality) }}Let $X$ be a random variable on measure space $(\Omega,\mathcal{F},\mathbb{P})$. If $X\in L^1(\mathbb{P})$ and $\phi:I\to\mathbb{R}$ is convex on some $I\subset\mathbb{R}$: for all $x,y\in I$ and $t\in[0,1]$,
\[\phi(tx+(1-t)y)\leq t\phi(x)+(1-t)\phi(y).\]
Then
\[\phi(\E[X])\leq\E[\phi(X)].\]
\end{prop}

\begin{prop}
Let $X$ and $Y$ be random variables on measure space $(\Omega,\mathcal{F},\mathbb{P})$ and $p,q\in[1,\infty]$ be such that $pq=p+q$.
\begin{itemize}
    \item \textup{\textbf{(Hölder's Inequality)}} If $X\in L^p(\mathbb{P})$ and $Y\in L^q(\mathbb{P})$, then $\norm{XY}_1\leq\norm{X}_p\norm{Y}_q.$
    \item \textup{\textbf{(Minkowski's Inequality)}} If $X\in L^p(\mathbb{P})$ and $Y\in L^p(\mathbb{P})$, then $\norm{X+Y}_{p}\leq\norm{X}_p\norm{Y}_p.$
\end{itemize}
\end{prop}

\begin{prop}
Let $X$ be a random variable on measure space $(\Omega,\mathcal{F},\mathbb{P})$.
\begin{itemize}
    \item \textup{\textbf{(Markov's Inequality)}} If $X$ is non-negative and $t\geq0$, then $t\mathbb{P}(X\geq t)\leq\mathbb{E}(X).$
    \item \textup{\textbf{(Chebyshev's Inequality)}} For any $t\geq0$, $t^2\mathbb{P}(\abs{X-\mathbb{E}(X)}\geq t)\leq\Var(X).$
    \item \textup{\textbf{(Chernoff bound)}} Given $m_X(\lambda)=\mathbb{E}(e^{\lambda X})$ the moment-generating function of $X$ and $\lambda>0$,
    \[e^{\lambda t}\mathbb{P}(X\geq t)\leq m_X(\lambda).\]
\end{itemize}
\end{prop}

\subsection{Independence and tail events}

\begin{prop}The followings are sufficient conditions for independence.\begin{itemize}
    \item If $\pi$-systems $\Pi_1$ and $\Pi_2$ are  independent, then $\mathcal{A}_1=\sigma(\Pi_1)$ and $\mathcal{A}_2=\sigma(\Pi_2)$ are independent.
    \item If $\mathbb{P}(X_1\leq t_1\textrm{ and }X_2\leq t_2)=\mathbb{P}(X_1\leq t_1)\mathbb{P}(X_2\leq t_2)$ for all $x_1,x_2\in\overline{\mathbb{R}}$, then $X_1,X_2$ are independent random variables.
    \item Let $f_1$ and $f_2$ be measurable functions $\mathbb{R}\to\mathbb{R}$ and $X_1,X_2$ independent random variables. Then $f_2(X_1)$ and $f_2(X_2)$ are independent.
\end{itemize}
\end{prop}

\begin{prop}
The followings are consequences of independence.\begin{itemize}
    \item If $X_1$ and $X_2$ be independent random variables with laws $\mu_1$ and $\mu_2$, then $(X_1,X_2)$ has law $\mu_1\times\mu_2$.
    \item For independent random variables $X_1$ and $X_2$, if $X_1\geq0$ or $\E(\abs{X_i})\leq\infty$ for $i=1,2$, then
    \[\E(X_1X_2)=\E(X_1)\E(X_2).\]
\end{itemize}
\end{prop}

\begin{prop}\textup{\textbf{(Hoeffding bound)}} Let $X_1,X_2,\cdots,X_n$ be a sequence of independent random variables such that $X_i\in[a_i,b_i]$ almost surely for all $i\in[1,n]$. Then for any $t\geq0$,
\[\mathbb{P}\left(\sum_{i=1}^n(X_i-\E(X_i))\geq t\right)\leq\exp{\frac{-Ct^2}{\sum_{i=1}^n(b_i-a_i)^2}}\]
for some constant $C$.
\end{prop}

\begin{prop}Let $(\Omega,\mathcal{F},\mathbb{P})$ be the measure space and $\{A_n\}_{n\geq1}$ a sequence of events.\begin{itemize}
    \item\textup{\textbf{(Borel-Cantelli Lemma I)}} If $\sum_{n\geq1}\mathbb{P}(A_n)<\infty$, then
\[\mathbb{P}(\limsup_{n\to\infty}A_n)=\mathbb{P}(\omega:\omega\in A\textrm{ infinitely often})=0.\]
    \item\textup{\textbf{(Borel-Cantelli Lemma II)}} If $\{A_n\}$ is pairwise independent and $\sum_{n\geq1}\mathbb{P}(A_n)=\infty$, then
    \[\mathbb{P}(\limsup_{n\to\infty}A_n)=1.\]
\end{itemize}
\end{prop}

\begin{remark}\textup{\textbf{Infinite Monkey Theorem:}} Let
\end{remark}

\begin{prop}\textup{\textbf{(Kolmogorov's 0-1 Law)}} Let $\{X_i\}_{i\geq1}$ be a sequence of independent random variables and $\mathcal{T}$ the tail $\sigma$-algebra of $\{X_i\}$. Then $\mathcal{T}$ is trivial: $\mathbb{P}(A)=0$ or $1$ for all $A\in\mathcal{T}$.
\end{prop}

\begin{remark}
Consider the collection of events $\mathcal{E}$ that the occurrence is invariant under finite permutation. $\mathcal{E}$ is a $\sigma$-algebra called exchangeable $\sigma$-algebra. A stronger version of the above theorem is \textbf{Hewitt-Savage 0-1 Law}:\smallbreak
\textit{Let $\{X_i\}_{i\geq1}$ be a sequence of independent and identically distributed random variables and $\mathcal{E}$ the exchangeable $\sigma$-algebra. Then $\mathcal{E}$ is trivial: $\mathbb{P}(A)=0$ or $1$ for all $A\in\mathcal{T}$.}
\end{remark}

\subsection{Convergence of random variables}

\begin{prop}
The following statements regarding the convergence of random variables holds:\begin{itemize}
    \item If $X_n\xrightarrow{a.s.}X$ or $X_n\xrightarrow{L^p}X$ for some $p$, then $X_n\xrightarrow{\mathbb{P}}X$. If $X_n\xrightarrow{\mathbb{P}}X$ then $X_n\xrightarrow{d}X$.
    \item If $X_n\xrightarrow{L^p}X$ for some $p$, $X_n\xrightarrow{\mathbb{P}}X$, or the sum $\sum_{n\geq1}\mathbb{P}(\abs{X_n-X}>\epsilon)<\infty$ for any $\epsilon>0$, then there is a subsequence $X_{n_k}\xrightarrow{a.s.}X$.
    \item If $X_n\xrightarrow{d}C$ for some constant $C$ then $X_n\xrightarrow{\mathbb{P}}X$.
    \item If $\{X_n\}$ is uniformly integrable, then $X_n\xrightarrow{a.s.}X$ implies $X_n\xrightarrow{L^1}X$; if $\{X_n\}$ is uniformly bounded, then $X_n\xrightarrow{\mathbb{P}}X$ implies $X_n\xrightarrow{L^p}X$ for all $p$.
    \item If $1\leq q\leq p$, then $X_n\xrightarrow{L^p}X$ implies $X_n\xrightarrow{L^q}X$.
    \item $X_n\xrightarrow{d}X$ if and only if $X_n\xrightarrow{weak}X$.
\end{itemize}
Or to put in a diagram:
\begin{center}\begin{tikzcd}
\begin{matrix}
    L^p\Rightarrow L^q\\1\leq q\leq p 
\end{matrix} \arrow[rd, Rightarrow] \arrow[dd, "subsequence" description, bend left]       &                                                                                                                      &                                                            &    \\
                                                                                  & \mathbb{P} \arrow[r, Rightarrow, bend right] \arrow[lu, "\textrm{uniform bounded}"', bend right] \arrow[ld, "\textrm{subsequence or sum}", bend left] & d \arrow[r, Leftrightarrow] \arrow[l, "X_n\xrightarrow{d} C"', bend right] & weak\\
a.s. \arrow[ru, Rightarrow] \arrow[uu, "\textrm{uniform integrable (to }L^1\textrm{)}", bend left=49] &                                                                                                                      &                                                            & 
\end{tikzcd}\end{center}
\end{prop}

\begin{prop}\textup{\textbf{(Slutsky's Theorem) }}If $X_n\xrightarrow{d}X$ and $Y_n\xrightarrow{\mathbb{P}}c$ for some constant $c$, then $X_n+Y_n\xrightarrow{d}X+c$ and $X_nY_n\xrightarrow{d}Xc$.
\end{prop}

\begin{prop}\textup{\textbf{(Portemanteau lemmas)}} Let $(\mathcal{S},\rho)$ be a polish space and $\{\mu_n\}$ a sequence of probability measures on $\mathcal{S}$, then the following statements are equivalent.\begin{itemize}
    \item $\mu_n\xrightarrow{weak}\mu$.
    \item For every bounded $f:\mathcal{S}\to\mathbb{R}$ that is Lipschitz continuous: for any $x,y\in\mathcal{S}$ there exists $L<\infty$ such that $\abs{f(x)-f(y)}\leq L\rho(x,y)$,
    \[\int_\mathcal{S}f\,d\mu_n\to\int_\mathcal{S}f\,d\mu.\]
    \item $\limsup_{n\to\infty}\mu_n(F)\leq\mu(F)$ and $\liminf_{n\to\infty}\mu_n(E)\geq\mu(A)$ for every closed $F$ and open $E$.
    \item $\mu_n(A)\to\mu(A)$ for all measurable $A\subset\mathcal{S}$ with $\mu(\partial A)=0$.
    \item For every bounded measurable $f:\mathcal{S}\to\mathbb{R}$ that is continuous almost everywhere with respect to $\mu$,
    \[\int_\mathcal{S}f\,d\mu_n\to\int_\mathcal{S}f\,d\mu.\]
\end{itemize}
\end{prop}

\begin{remark}
It follows that if $\int_\mathcal{S}f\,d\mu=\int_\mathcal{S}f\,d\nu$ for probability measures $\mu$ and $\mu$  and all bounded continuous $f:\mathcal{S}\to\mathbb{R}$, then $\mu=\nu$. Indeed, by Portemanteau lemmas $\mu$ and $\nu$ thus agree on all open sets, thus by Dynkin's $\pi$-$\lambda$ Theorem the uniqueness follows.
\end{remark}

\subsection{Laws of large numbers}

\begin{prop}\textup{\textbf{($L^2$ Weak Law) }}Let $\{X_i\}_{i\geq1}$ be a sequence of uncorrelated and identically distributed random variables with finite variance for all $i$. Then
\[\frac{1}{n}\sum_{i=1}^nX_n\xrightarrow{L^2}\E(X_1)\]
\end{prop}

\begin{remark}\textup{\textbf{Coupon Collector's Problem:}} Let
\end{remark}

\begin{prop}\textup{\textbf{(Weak Law of Triangular Array) }}Let $a_n$ be a sequence of positive integers such that $a_n\to\infty$. Let $\{X_{n,k}\}_{1\leq k\leq n}$ be a sequence of independent random variables and truncation $\overline{X}_{n,k}=X_{n,k}\1_{\{\abs{X_{n,k}\leq a_n}\}}$. If as $n\to\infty$\begin{itemize}
    \item $\sum_{k=1}^n\mathbb{P}(\abs{X_{n,k}}>a_n)\to0$, and
    \item $b_n^{-2}\sum_{k=1}^n\E(\overline{X}_{n,k}^2)\to0$,
\end{itemize}
then
\[b_n^{-1}\sum_{k=1}^n(X_{n,k}-\E(\overline{X}_{n,k}))\xrightarrow{\mathbb{P}}0\]
\end{prop}

\begin{prop}\textup{\textbf{(Weak Law of Large Numbers) }}Let $\{X_i\}_{i\geq1}$ be a sequence of independent and identically distributed random variables such that $n\mathbb{P}(\abs{X_1}>n)\to0$ as $n\to\infty$. Then
\[\frac{1}{n}\sum_{i=1}^n X_i\xrightarrow{\mathbb{P}}\E(X_1)\]
\end{prop}

\begin{prop}\textup{\textbf{(Strong Law of Large Numbers) }}Let $\{X_i\}_{i\geq1}$ be a sequence of pairwise independent and identically distributed random variables such that $\mathbb{E}(\abs{X_1})<\infty$. Then
\[\frac{1}{n}\sum_{i=1}^nX_i\xrightarrow{a.s.}\mathbb{E}(X_1).\]
\end{prop}

\subsection{Tightness and characteristic functions}

\begin{prop}
Let $\{X_n\}_{n\geq1}$ be a sequence of random variables.
\begin{itemize}
\item If $\{X_n\}$ be a tight family and $\{c_n\}_{n\geq1}$ a sequence of constants converging to 0. Then,\begin{itemize}
    \item \textup{\textbf{(Helly's Selection Theorem)}} there is a subsequence $X_{n_k}\xrightarrow{d}X$, and
    \item $c_nX_n\xrightarrow{\mathbb{P}}0$.
\end{itemize}
\item Conversely if $X_n\xrightarrow{d}X$, then $\{X_n\}$ is a tight family.
\end{itemize}
\end{prop}

\begin{prop}
Let $\{\mu_n\}_{n\geq1}$ be a sequence of probability measures on a Polish space $(\mathcal{S},\rho)$.\begin{itemize}
    \item If $\mu_n\xrightarrow{weak}\mu$, then $\{\mu_n\}$ is tight.
    \item \textup{\textbf{(Prokhorov's Theorem)}} If $\{\mu_n\}$ is tight, then there is a subsequence $\mu_{k_n}\xrightarrow{weak}\mu$.
\end{itemize}
\end{prop}

\begin{prop}
The following properties of characteristic functions hold.
\begin{itemize}
    \item $\abs{\phi(t)}\leq\E(\abs{e^{itX}})=1$.
    \item $\phi(t)$ is uniformly continuous: $\abs{\phi(t+h)-\phi(t)}\leq\E(\abs{e^{ihX}-1})$.
    \item if $X$ and $Y$ are independent, then $\phi_{X+Y}=\phi_X\phi_Y$.
\end{itemize}
\end{prop}

\begin{prop}
Let $\phi$ be a function and $t_1<t_2<\cdots<t_n$. Let $A_{ij}=\phi(t_i-t_j)$ be a $n\times n$ matrix.\begin{itemize}
    \item If $\phi$ is a characteristic function of some random variable, then the following properties hold:\begin{itemize}
        \item A is Hermitian: $A^*=\overline{A^T}=A$.
        \item A is positive semi-definite: for any $\forall v\in\mathbb{C}^n:v^TAv\geq0$.
        \item $\phi$ is continuous at 0 with $\phi(0)=1$.
    \end{itemize}
    \item \textup{\textbf{(Bochner's Theorem)}} Conversely if $\phi$ satisfies the listed properties, then it is the characteristic function of some random variable.
\end{itemize}
\end{prop}

\begin{remark}
example: cf of N(mu,sigma)
\end{remark}

\begin{prop}\textup{\textbf{(Inversion Formula)}} For any random variable $X$ and $\theta>0$ define
\[f_\theta(X)=\frac{1}{2\pi}\int_{-\infty}^\infty e^{itX-\theta t^2}\phi_X(t)\,dt.\]
Then for any bounded continuous function $g:\R\to\R$,
\[\E[g(X)]=\lim_{\theta\to0}\int_{-\infty}^\infty g(x)f_\theta(x)\,dx.\]
\end{prop}

\begin{remark}
Two random variables have the same law if and only if they have the same characteristic function. Indeed, if $X$ and $Y$ have the same law, then they have the same $k$-th moment $\E[X^k]=\E[Y^k]$ for all $k$. The characteristic function $\E[e^{iXt}]$ can be decomposed to sum of $k$th moments:
\begin{align*}
\E[e^{iXt}]&=\E(1+iXt-\frac{X^2t^2}{2!}+\cdots+\frac{i^nX^nt^n}{n!}+\cdots)\\&=1+it\E[X]-\frac{t^2\E[X^2]}{2!}+\cdots+\frac{t^n\E[X^n]}{n!}+\cdots.
\end{align*}
On the other hand, if $X$ and $Y$ have the same characteristic function, then by Inversion Formula they have the same law.
\end{remark}

\begin{remark}
Suppose $\int_{-\infty}^\infty\abs{\phi_X(t)}\,dt<\infty$, then by Inversion Formula the probability density function of $X$ is given by
\[f(x)=\frac{1}{2\pi}\int_{-\infty}^\infty e^{-itx}\phi_X(t)\,dt.\]
\end{remark}

\begin{prop}\textup{\textbf{(Lévy's Continuity Theorem)}} A sequence of random variables $\{X_n\}$ converges to $X$ in distribution if and only if their characteristic functions $\{\phi_{X_n}\}$ converges to $\phi_X$ pointwise.
\end{prop}

\begin{remark}
\textup{\textbf{Cramér-Wold Theorem:}} Let $\{X_n\}_{n\geq1}$ be a sequence of random vectors in $\mathbb{R}^n$, then $X_n\xrightarrow{d}X$ if and only if $tX_n\xrightarrow{d}tX$ for any $t\in\mathbb{R}^n$. Indeed, one direction is a consequence of the fact that all finite-dimensional linear maps are continuous; the other direction follows from Lévy's Continuity Theorem.
\end{remark}

\begin{prop}\textup{\textbf{(Central Limit Theorem)}} Let $\{X_i\}_{i\geq1}$ be a sequence of independent and identically ditributed random variables with $\E(X_1)=\mu$ and $\Var(E_1)=\sigma^2<\infty$. Then
\[S_n=\frac{\sum_{i=1}^nX_i-\mu}{\sigma\sqrt{n}}\xrightarrow{d}\mathcal{N}(0,1).\]
\end{prop}

\begin{remark}
The above Central Limit Theorem is the classical result. A stronger one that reduces the identical distribution condition is \textbf{Lindeberg-Feller Central Limit Theorem}:\smallbreak
\textit{Let $\{k_n\}_{n\geq1}$ be a sequence of positive integers with $k_n\to\infty$. Let $\{X_{n,i}\}_{1\leq i\leq k_n}$ be a collection of independent random variables and denote $\mu_{n,i}=\E(X_{n,i})$, $\sigma_{n,i}^2=\Var(X_{n,i})$, and $S_n^2=\sum_{i=1}^{k_n}\sigma_{n,i}^2$. If $\{X_{n,i}\}$ satisfies Lindeberg condition:}
\[\forall\epsilon>0:\frac{1}{S_n^2}\sum_{i=1}^{k_n}\E\left[(X_{n,i}-\mu_{n,i})^2\textbf{1}_{\{\abs{X_{n,i}-\mu_{n,i}}\geq\epsilon S_n\}}\right]\to0.\]
\textit{Then}
\[\frac{1}{S_n}\sum_{i=1}^{k_n}(X_{n,i}-\mu_{n,i})\xrightarrow{d}\mathcal{N}(0,1).\]
As a corollary to the Lindeberg-Feller is \textbf{Lyapunov Central Limit Theorem}:\smallbreak
\textit{Let $\{X_i\}_{i\geq1}$ be a sequence of independent random variables with $\E(X_i)=\mu_i$, $\sigma_i^2=\Var(X_i)<\infty$ and $S_n^2=\sum_{i=1}^n\sigma_i^2$. If $\{X_i\}$ satisfies Lyapunov condition:}
\[\exists\delta>0:\frac{1}{S_n^{2+\delta}}\sum_{i=1}^n\E\left[\abs{X_i-\mu_i}^{2+\delta}\right]\to0.\]
\textit{Then}
\[\frac{1}{S_n}\sum_{i=1}^n(X_i-\mu_i)\xrightarrow{d}\mathcal{N}(0,1).\]
\end{remark}

\section{Brownian Motion}

\section{Ergodic Theory}



\end{document}